{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756ff76-6473-4da1-ab20-e28c48a003f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# This notebook is a SOE Assessment Test Analysis.                            #\n",
    "###############################################################################\n",
    "# Core stuff\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Data stuff\n",
    "import pandas as pd # Data analysis\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Pretty printing stuff\n",
    "from IPython.display import display, HTML\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_area_auto_adjustable\n",
    "\n",
    "# Configuration (initial setup)\n",
    "with open('config.json', 'r') as file:\n",
    "     config = json.load(file)\n",
    "\n",
    "test = config['test']\n",
    "country = config['country']\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f1490-99a8-437c-92d4-889e5802e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_to_df(filename):\n",
    "    \"\"\"Loads an Excel filename to a Pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str, required\n",
    "        The filename of the excel file to load\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    file_path = Path(filename)\n",
    "    file_extension = file_path.suffix.lower()[1:]\n",
    "\n",
    "    if file_extension == 'xlsx':\n",
    "        df_student_results = pd.read_excel(filename, index_col=None, header=0, engine='openpyxl')\n",
    "    elif file_extension == 'xls':\n",
    "        df_student_results = pd.read_excel(filename, index_col=None, header=0)\n",
    "    elif file_extension == 'csv':\n",
    "        df_student_results = pd.read_csv(filename, index_col=None, header=0)\n",
    "    else:\n",
    "        raise Exception(\"File not supported\")\n",
    "\n",
    "    return df_student_results\n",
    "\n",
    "def save(df, filename, as_string):\n",
    "    # Generate the output file path with -anonymized added\n",
    "    file_root, file_ext = os.path.splitext(filename)\n",
    "    filename_as_string = f\"{file_root}{as_string}{file_ext}\"\n",
    "    \n",
    "    # Save the anonymized data to the new file\n",
    "    if file_ext in ['.csv']:\n",
    "        df.to_csv(filename_as_string, index=False)\n",
    "        print(f\"Saving {filename_as_string}\")\n",
    "    elif file_ext in ['.xls', '.xlsx']:\n",
    "        df.to_excel(filename_as_string, index=False)\n",
    "        print(f\"Saving {filename_as_string}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec0037-c9b7-4931-a93a-03a718a8bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Response Sheet                                                              #\n",
    "###############################################################################\n",
    "\n",
    "# Load a single SOE Assessment workbook (for testing,)\n",
    "# in particular the sheet with the raw data\n",
    "local_path = os.path.abspath('/mnt/h/Development/Pacific EMIS/repositories-data/pacific-emis-exams/')\n",
    "filename = os.path.join(local_path, 'RMI/soe-load-files/e33a8ebb-2d79-411f-a7a6-d3f4c443906b.xlsx')\n",
    "\n",
    "df_student_results = load_excel_to_df(filename)\n",
    "print('df_student_results')\n",
    "display(df_student_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b93d95-b1d7-4f93-aa8c-b2bb5f8dbb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a unique fictitious name\n",
    "def generate_fictitious_name(existing_names):\n",
    "    while True:\n",
    "        name = ''.join(random.choices(string.ascii_uppercase, k=5))\n",
    "        if name not in existing_names:\n",
    "            existing_names.add(name)\n",
    "            return name\n",
    "\n",
    "# Initialize sets to keep track of used fictitious names\n",
    "student_names = set()\n",
    "teacher_names = set()\n",
    "\n",
    "# Create mappings for student and teacher names\n",
    "student_name_mapping = {name: generate_fictitious_name(student_names) for name in df_student_results['STUDENTNAME'].unique()}\n",
    "teacher_name_mapping = {name: generate_fictitious_name(teacher_names) for name in df_student_results['TEACHERNAME'].unique()}\n",
    "\n",
    "# Replace the names in the DataFrame\n",
    "df_student_results['STUDENTNAME'] = df_student_results['STUDENTNAME'].map(student_name_mapping)\n",
    "df_student_results['TEACHERNAME'] = df_student_results['TEACHERNAME'].map(teacher_name_mapping)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "display(df_student_results)\n",
    "\n",
    "save(df_student_results, filename, '-anonymized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06961328-346c-45ee-add4-99e79576e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract item information from the column name\n",
    "def extract_item_info(column_name):\n",
    "    match = re.match(r\"ITEM_(\\d+)_([A-Z0-9]+)_([A-Z]+)$\", column_name)\n",
    "    if match:\n",
    "        item_number = match.group(1)\n",
    "        item_details = match.group(2)\n",
    "        correct_answer = match.group(3)[0]\n",
    "        return item_number, item_details, correct_answer\n",
    "    return None, None, None\n",
    "\n",
    "# Initialize lists to store extracted information\n",
    "item_numbers = []\n",
    "item_details = []\n",
    "correct_answers = []\n",
    "\n",
    "# Convert column names to uppercase\n",
    "df_student_results.columns = [col.upper() for col in df_student_results.columns]\n",
    "\n",
    "# Extract item information from each item column\n",
    "for column in df_student_results.columns:\n",
    "    if column.startswith('ITEM_'):\n",
    "        item_number, item_detail, correct_answer = extract_item_info(column)\n",
    "        item_numbers.append(item_number)\n",
    "        item_details.append(item_detail)\n",
    "        correct_answers.append(correct_answer)\n",
    "\n",
    "# Create a DataFrame with the extracted item information\n",
    "item_info_df = pd.DataFrame({\n",
    "    'Item Number': item_numbers,\n",
    "    'Item Details': item_details,\n",
    "    'Correct Answer': correct_answers\n",
    "})\n",
    "\n",
    "# Display the item information DataFrame\n",
    "display(item_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d8394-1d14-49d3-9318-89982215bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the student's answer is correct\n",
    "def is_correct_answer(student_answer, correct_answer):\n",
    "    return 1 if str(student_answer).strip().upper() == correct_answer else 0\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "df_results = df_student_results.copy()\n",
    "\n",
    "# Calculate the score for each item\n",
    "for column in df_student_results.columns:\n",
    "    if column.startswith('ITEM_'):\n",
    "        item_number, item_detail, correct_answer = extract_item_info(column)\n",
    "        if correct_answer:\n",
    "            df_results[f'{column}_CORRECT'] = df_student_results[column].apply(is_correct_answer, correct_answer=correct_answer)\n",
    "\n",
    "# Calculate the total score for each student\n",
    "df_results['TOTAL_SCORE'] = df_results.filter(like='_CORRECT').sum(axis=1)\n",
    "\n",
    "# Display the results DataFrame\n",
    "display(df_results)\n",
    "save(df_results, filename, '-with-scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591e0c7-6b0d-4692-8af2-bf6a8109d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns containing 'SCORE' in their names but excluding 'MAXSCORE', 'SCORE_RATIO', 'SCORE_TOTAL', and 'SCORE_TOTAL_MAX'\n",
    "score_columns = [col for col in df_results.columns if 'CORRECT' in col]\n",
    "\n",
    "# Summary statistics for each score column\n",
    "score_statistics = df_results[score_columns].describe()\n",
    "\n",
    "# Display the statistics\n",
    "print(\"Summary Statistics for SCORE Columns:\")\n",
    "display(score_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff56ab8e-2547-450f-986c-926c70d77c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Calculate Cronbach's alpha\n",
    "################################################################################\n",
    "\n",
    "# Subset the data to include only the score columns\n",
    "df_scores = df_results[score_columns]\n",
    "\n",
    "# Calculate the number of items\n",
    "n_items = len(score_columns)\n",
    "print(f\"Number of items: {n_items}\")\n",
    "\n",
    "# Calculate the variance for each item\n",
    "item_variances = df_scores.var(axis=0, ddof=1)\n",
    "#print(f\"Item variances: {item_variances.head()}\")\n",
    "print(f\"Sum of item variances: {item_variances.sum()}\")\n",
    "\n",
    "# Calculate the total score for each participant\n",
    "total_scores = df_scores.sum(axis=1)\n",
    "#print(f\"Total scores: {total_scores.head()}\")\n",
    "# Calculate the standard deviation of totals\n",
    "std_dev_totals = total_scores.std()\n",
    "print(f\"Standard Deviation of Totals: {std_dev_totals}\")\n",
    "\n",
    "# Calculate the variance of the total scores\n",
    "total_score_variance = total_scores.var(ddof=1)\n",
    "print(f\"Total scores variance: {total_score_variance}\")\n",
    "\n",
    "# Calculate Cronbach's alpha\n",
    "cronbach_alpha = (n_items / (n_items - 1)) * (1 - (item_variances.sum() / total_score_variance))\n",
    "\n",
    "print(f\"Cronbach's Alpha: {cronbach_alpha}\")\n",
    "\n",
    "# Calculate SEM\n",
    "sem = std_dev_totals * (1 - cronbach_alpha) ** 0.5\n",
    "print(f\"Standard Error of Measurement: {sem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9e1b0-a450-47ea-b8ce-8828cfb2fdab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define the item column to analyze\n",
    "item_column = 'ITEM_001_MS0601010101E_DDD'\n",
    "\n",
    "# Function to extract the correct answer from the item column name\n",
    "def extract_correct_answer(column_name):\n",
    "    match = re.match(r\"ITEM_(\\d+)_([A-Z0-9]+)_([A-Z]{3})$\", column_name)\n",
    "    if match:\n",
    "        correct_answer = match.group(3)[0]  # Take only the first letter of the correct answer\n",
    "        return correct_answer\n",
    "    return None\n",
    "\n",
    "# Extract the correct answer for the item\n",
    "correct_answer = extract_correct_answer(item_column)\n",
    "\n",
    "# Function to count the number of candidates for each possible answer\n",
    "def count_candidates(item_column):\n",
    "    answers = ['A', 'B', 'C', 'D']\n",
    "    counts = {answer: 0 for answer in answers}\n",
    "    \n",
    "    for answer in df_results[item_column]:\n",
    "        if str(answer).strip().upper() in counts:\n",
    "            counts[str(answer).strip().upper()] += 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Count the number of candidates for each possible answer\n",
    "counts = count_candidates(item_column)\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "df_item_analysis = pd.DataFrame({\n",
    "    'Answer': list(counts.keys()),\n",
    "    'Number of Candidates': list(counts.values())\n",
    "})\n",
    "\n",
    "# Calculate the percentage of candidates for each answer\n",
    "total_candidates = df_item_analysis['Number of Candidates'].sum()\n",
    "df_item_analysis['Percentage of Candidates'] = (df_item_analysis['Number of Candidates'] / total_candidates) * 100\n",
    "\n",
    "# Add a column to indicate the correct answer\n",
    "df_item_analysis['Is Correct'] = df_item_analysis['Answer'] == correct_answer\n",
    "\n",
    "# Display the DataFrame with conditional formatting\n",
    "def highlight_correct(s):\n",
    "    return ['background-color: lime' if is_correct else '' for is_correct in s]\n",
    "\n",
    "df_styled = df_item_analysis.style.apply(highlight_correct, subset=['Is Correct'])\n",
    "df_styled = df_styled.format({'Percentage of Candidates': '{:.2f}%'})\n",
    "\n",
    "display(df_styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1aa75e-2644-4489-bcd1-dd7aec26de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pie chart based on the df_item_analysis DataFrame\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the pie chart\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    df_item_analysis['Number of Candidates'],\n",
    "    labels=df_item_analysis['Answer'],\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=['blue', 'orange', 'green', 'red']\n",
    ")\n",
    "\n",
    "# Highlight the correct answer\n",
    "correct_answer_index = df_item_analysis[df_item_analysis['Answer'] == correct_answer].index[0]\n",
    "wedges[correct_answer_index].set_edgecolor('lime')\n",
    "wedges[correct_answer_index].set_linewidth(2)\n",
    "\n",
    "# Add a title\n",
    "plt.title('Distribution of Candidates\\' Answers')\n",
    "\n",
    "# Display the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0919f-6ad2-47b2-9454-4f9a835f3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assuming df_results is already defined and contains the necessary data\n",
    "\n",
    "# Function to extract expected difficulty from the item column name\n",
    "def extract_expected_difficulty(column_name):\n",
    "    match = re.match(r\"ITEM_(\\d+)_([A-Z0-9]+)([EMH])_([A-Z]{3})$\", column_name)\n",
    "    if match:\n",
    "        expected_difficulty = match.group(3).lower()  # Extract and convert to lowercase\n",
    "        if expected_difficulty == 'h':\n",
    "            return 'Hard'\n",
    "        elif expected_difficulty == 'e':\n",
    "            return 'Easy'\n",
    "        elif expected_difficulty == 'm':\n",
    "            return 'Moderate'\n",
    "        else:\n",
    "            return 'Error'\n",
    "    return None\n",
    "\n",
    "# Function to calculate assessed difficulty\n",
    "def calculate_assessed_difficulty(correct_answers, total_answers):\n",
    "    percentage_correct = (correct_answers / total_answers) * 100\n",
    "    if percentage_correct < 33.33:\n",
    "        return 'Hard'\n",
    "    elif percentage_correct > 66.66:\n",
    "        return 'Easy'\n",
    "    else:\n",
    "        return 'Moderate'\n",
    "\n",
    "# Initialize lists to store the difficulty levels\n",
    "expected_difficulties = []\n",
    "assessed_difficulties = []\n",
    "\n",
    "# Iterate through each item column to extract expected difficulty and calculate assessed difficulty\n",
    "for column in df_results.columns:\n",
    "    if column.startswith('ITEM_') and column.endswith('_CORRECT'):\n",
    "        # Extract expected difficulty\n",
    "        base_column_name = column[:-8]  # Remove '_CORRECT' to get the base column name\n",
    "        expected_difficulty = extract_expected_difficulty(base_column_name)\n",
    "        \n",
    "        # Calculate assessed difficulty\n",
    "        total_answers = len(df_results)\n",
    "        correct_answers = df_results[column].sum()\n",
    "        assessed_difficulty = calculate_assessed_difficulty(correct_answers, total_answers)\n",
    "        \n",
    "        expected_difficulties.append(expected_difficulty)\n",
    "        assessed_difficulties.append(assessed_difficulty)\n",
    "\n",
    "# Create a DataFrame with the difficulty comparison report\n",
    "df_difficulty_comparison = pd.DataFrame({\n",
    "    'Item': [col[:-8] for col in df_results.columns if col.startswith('ITEM_') and col.endswith('_CORRECT')],\n",
    "    'Expected Difficulty': expected_difficulties,\n",
    "    'Assessed Difficulty': assessed_difficulties\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "#df_styled = df_difficulty_comparison.style.applymap(lambda x: 'background-color: lightgreen' if x == 'easy' else ('background-color: lightcoral' if x == 'hard' else 'background-color: lightyellow'), subset=['Assessed Difficulty'])\n",
    "df_difficulty_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476d04b-38a0-4113-8876-ce64ef55a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_results is already defined and contains the necessary data\n",
    "\n",
    "# Define the item column to analyze\n",
    "item_column = 'ITEM_001_MS0601010101E_DDD_CORRECT'\n",
    "\n",
    "# Calculate the total score for each student\n",
    "df_results['TOTAL_SCORE'] = df_results.filter(like='_CORRECT').sum(axis=1)\n",
    "\n",
    "# Rank students based on their total scores\n",
    "df_results['RANK'] = df_results['TOTAL_SCORE'].rank(ascending=False, method='first')\n",
    "#print(\"df_resuts ranked:\")\n",
    "#display(df_results)\n",
    "\n",
    "# Calculate the number of students in each group\n",
    "num_students = len(df_results)\n",
    "num_top_bottom = math.ceil(num_students * 0.27)\n",
    "num_middle = num_students - (num_top_bottom*2)\n",
    "\n",
    "# Sort the dataframe by rank\n",
    "df_sorted = df_results.sort_values(by='RANK')\n",
    "save(df_sorted, filename, '-sorted')\n",
    "\n",
    "# Select top and bottom 27% of students (and middle 46%)\n",
    "top_27_percent = df_sorted.head(num_top_bottom)\n",
    "bottom_27_percent = df_sorted.tail(num_top_bottom)\n",
    "middle_46_percent = df_sorted.iloc[num_top_bottom:-num_top_bottom]\n",
    "\n",
    "# Calculate the rate of correct answers for the top, middle, and bottom groups\n",
    "top_group_correct = top_27_percent[item_column].sum()\n",
    "middle_group_correct = middle_46_percent[item_column].sum()\n",
    "bottom_group_correct = bottom_27_percent[item_column].sum()\n",
    "total_group_correct = top_group_correct + middle_group_correct + bottom_group_correct\n",
    "print(f\"top_group_correct: {top_group_correct}\")\n",
    "print(f\"middle_group_correct: {middle_group_correct}\")\n",
    "print(f\"bottom_group_correct: {bottom_group_correct}\")\n",
    "print(f\"total_group_correct: {total_group_correct}\")\n",
    "\n",
    "top_group_correct_rate = top_group_correct / num_top_bottom\n",
    "middle_group_correct_rate = middle_group_correct / num_middle\n",
    "bottom_group_correct_rate = bottom_group_correct / num_top_bottom\n",
    "total_group_correct_rate = total_group_correct / num_students\n",
    "print(f\"top_group_correct_rate: {top_group_correct_rate}\")\n",
    "print(f\"middle_group_correct_rate: {middle_group_correct_rate}\")\n",
    "print(f\"bottom_group_correct_rate: {bottom_group_correct_rate}\")\n",
    "print(f\"total_group_correct_rate: {total_group_correct_rate}\")\n",
    "\n",
    "# Compute the discrimination index\n",
    "discrimination_index = top_group_correct_rate - bottom_group_correct_rate\n",
    "\n",
    "# Compute the discrimination index\n",
    "discrimination_index = top_group_correct_rate - bottom_group_correct_rate\n",
    "\n",
    "discrimination_index\n",
    "\n",
    "# NOTE: that the final correct rates and thus the discrimination index will vary a little\n",
    "# from live Pacific EMIS. The reason is in the ranking and thus the final selection of the \n",
    "# top and bottom students. The EMIS will sort by exam candidate ID and essentially randomly\n",
    "# cutoff at 27% (top and bottom) where you would generally get equiqually performing students.\n",
    "# Due to a lot of \"ties\" one has no choice to do this. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
