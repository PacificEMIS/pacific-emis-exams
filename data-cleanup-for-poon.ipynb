{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# This notebook was written to process, cleanup and attempt to retrieve       #\n",
    "# better student ID and school ID data from MIEMIS on a request from Poon     #\n",
    "# from Pacific Testing Center (OnlineSBA)                                     #\n",
    "# An ad-hoc support request that led to a more general and re-usable notebook #\n",
    "# soe-to-onlinesba.ipynb                                                      #\n",
    "###############################################################################\n",
    "\n",
    "# Import core stuff\n",
    "import json\n",
    "\n",
    "# Import Data stuff\n",
    "import pandas as pd # Data analysis\n",
    "import xlrd # excel \n",
    "import pyodbc # SQL DB\n",
    "\n",
    "# fuzz is used to compare TWO strings\n",
    "from fuzzywuzzy import fuzz\n",
    "# process is used to compare a string to MULTIPLE other strings\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Configuration\n",
    "with open('config.json', 'r') as file:\n",
    "     config = json.load(file)\n",
    "\n",
    "# It is important to keep the order of the cells since there are inplace \n",
    "# operations on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data into a cleanish DataFrame\n",
    "\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "f = os.path.join(cwd, 'data/RMI/poon-cleanup-request/M03 2019.xlsx')\n",
    "\n",
    "df_exams = pd.read_excel(f, index_col=None, header=0, engine='openpyxl')\n",
    "df_exams.dropna(how='all',inplace=True)\n",
    "df_exams.reset_index(drop=True, inplace=True)\n",
    "df_exams\n",
    "\n",
    "\n",
    "# investigate [nan, 'MH010787', None, 'MH009285', 'MH035753'] # They exists here!!!\n",
    "#df_exams[df_exams['STUDENTNAME'] == 'Rine Sam']\n",
    "#df_exams[df_exams['STUDENTNAME'] == 'Rine Sam'].STUDENTNAME\n",
    "\n",
    "#len(df_exams['STUDENTID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevant student enrollments from the database\n",
    "# there we have all known students loaded from census year after year\n",
    "enrol_year = 2019\n",
    "\n",
    "# Establish a database server connection\n",
    "conn = \"\"\"\n",
    "    Driver={{ODBC Driver 17 for SQL Server}};\n",
    "    Server={},{};\n",
    "    Database={};\n",
    "    authentication=SqlPassword;UID={};PWD={};\n",
    "    TrustServerCertificate=yes;\n",
    "    autocommit=True\n",
    "    \"\"\".format(config['server_ip'], config['server_port'], config['database'], config['uid'], config['pwd'])\n",
    "\n",
    "sql_conn = pyodbc.connect(conn)\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "\tstuCardID\n",
    "\t, CONCAT(stuGiven,' ',stuFamilyName) AS Student -- stuMiddleNames,' ',\n",
    "\t, stuGender\n",
    "\t, stuDoB\n",
    "\t, schNo\n",
    "\t, stueYear\n",
    "\tFROM Student_ S\n",
    "\tINNER JOIN StudentEnrolment_ SE ON S.stuID = SE.stuID\n",
    "\tWHERE stueYear = {0}\n",
    "\"\"\".format(enrol_year)\n",
    "                          \n",
    "df_student_enrol = pd.read_sql(query, sql_conn)\n",
    "df_student_enrol\n",
    "\n",
    "#df_student_enrol.head(3)\n",
    "#df_student_enrol.count()\n",
    "#df_student_enrol[df_student_enrol['Student'].str.contains('Ranny George', case=False)]\n",
    "\n",
    "# investigate [nan, 'MH010787', None, 'MH009285', 'MH035753'] \n",
    "# They existed here but pandas does not equate as there were spaces to trim!!!\n",
    "#df_student_enrol[df_student_enrol['stuCardID'] == 'MH035753']\n",
    "#df_student_enrol[df_student_enrol['stuCardID'] == 'MH035753'].Student\n",
    "#df_student_enrol[df_student_enrol['Student'].str.strip() == 'Rine Sam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge both the dirty exams data with the clean student enrollments dataset\n",
    "\n",
    "   \n",
    "# lower case to make join case insensitive (like SQL Server, the default collation of Pacific EMIS anyway)\n",
    "df_exams['STUDENTNAME'] = df_exams['STUDENTNAME'].str.lower()\n",
    "df_student_enrol['Student'] = df_student_enrol['Student'].str.lower()\n",
    "\n",
    "# Also need to trim spaces to make it exactly like the SQL Server join\n",
    "df_exams['STUDENTNAME'] = df_exams['STUDENTNAME'].str.strip()\n",
    "df_student_enrol['Student'] = df_student_enrol['Student'].str.strip()\n",
    "\n",
    "#df_exams_and_students = df_exams.set_index('STUDENTNAME').join(df_student_enrol.set_index('Student'), lsuffix='_caller', rsuffix='_other')\n",
    "df_exams_and_students = df_exams.merge(df_student_enrol, how='left', left_on='STUDENTNAME', right_on='Student', suffixes=('_from_exams', '_from_db'))\n",
    "df_exams_and_students\n",
    "\n",
    "#df_exams_and_students[df_exams_and_students[STUDENTID=='Ranny George']]\n",
    "#df_exams_and_students.loc['Ranny George']\n",
    "#df_exams_and_students.loc['RANNY GEORGE']\n",
    "#df_exams_and_students[df_exams_and_students[schNo=='MH000036']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the duplicates. This could be that there are two or more matches\n",
    "# of exams students into the clean DB students enrollments\n",
    "# (e.g. same name, different student, which one is it?)\n",
    "#df_exams_and_students_dups = df_exams_and_students[df_exams_and_students.index.duplicated(keep=False)]\n",
    "df_exams_and_students_dups = df_exams_and_students[df_exams_and_students['STUDENTNAME'].duplicated(keep=False)]\n",
    "df_exams_and_students_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the exact matches (i.e. exact name in exams data and the database)\n",
    "# not actually using this, included for observations. We'll be using fuzzy matching\n",
    "df_exact_matches = df_exams_and_students.dropna(how='all', subset=['stuCardID']) #subset=['stuCardID', 'stuGender', 'stuDoB', 'schNo', 'stueYear'])\n",
    "df_exact_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting differences with SQL\n",
    "# No longer needed\n",
    "#query2 = \"\"\"\n",
    "#SELECT * FROM [dbo].[StudentMISATData] AS SMISAT LEFT JOIN \n",
    "#(SELECT\n",
    "#\tstuCardID\n",
    "#\t, CONCAT(stuGiven,' ',stuFamilyName) AS Student -- stuMiddleNames,' ',\n",
    "# \t, stuGender\n",
    "# \t, stuDoB\n",
    "# \t, schNo\n",
    "# \t, stueYear\n",
    "# \tFROM Student_ S\n",
    "# \tINNER JOIN StudentEnrolment_ SE ON S.stuID = SE.stuID\n",
    "# \tWHERE stueYear = 2019) AS S ON SMISAT.STUDENTNAME = S.Student\n",
    "# \"\"\"   \n",
    "# df_student_enrol2 = pd.read_sql(query2, sql_conn)\n",
    "# df_student_enrol2\n",
    "\n",
    "# # investigate [nan, 'MH010787', None, 'MH009285', 'MH035753']\n",
    "# s = df_exams_and_students['stuCardID']\n",
    "# s.isna().sum()\n",
    "# l = list(s)\n",
    "# df_exams_and_students[df_exams_and_students['stuCardID'] == 'MH035753']\n",
    "# df_exams_and_students[df_exams_and_students['Student'] == 'Rine Sam']\n",
    "# df_exams_and_students[df_exams_and_students['STUDENTNAME'] == 'Rine Sam']\n",
    "\n",
    "# s2 = df_student_enrol2['stuCardID']\n",
    "# s2.isna().sum()\n",
    "# l2 = list(s2)\n",
    "\n",
    "# df_student_enrol2[df_student_enrol2['stuCardID'] == 'MH035753']\n",
    "# df_student_enrol2[df_student_enrol2['Student'] == 'Rine Sam']\n",
    "# df_student_enrol2[df_student_enrol2['STUDENTNAME'] == 'Rine Sam']\n",
    "\n",
    "#common_cols = list(set(df_exams_and_students.columns) & set(df_student_enrol2.columns))\n",
    "#common_cols\n",
    "\n",
    "#pd.merge(df_exams_and_students, df_student_enrol2, how='right', left_on='STUDENTNAME', right_on='Student')\n",
    "\n",
    "# Python code t get difference of two lists\n",
    "# Using set()\n",
    "# def Diff(li1, li2):\n",
    "#     return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))\n",
    " \n",
    "# # Driver Code\n",
    "# li1 = [10, 15, 20, 25, 30, 35, 40]\n",
    "# li2 = [25, 40, 35]\n",
    "# print(Diff(l, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resources http://jonathansoma.com/lede/algorithms-2017/classes/fuzziness-matplotlib/fuzzing-matching-in-pandas-with-fuzzywuzzy/\n",
    "# Scores: 100 is 100% matching\n",
    "#print(fuzz.ratio(\"ghislain hachey\", \"gislain hachey\")) # compares entire string in order\n",
    "#print(fuzz.partial_ratio(\"ghislain timbasal\", \"ghislain hachey\")) # compare subsection of the string\n",
    "#print(fuzz.token_sort_ratio(\"ghislain hachey\", \"hachey ghislain\")) # ignores work order\n",
    "#print(fuzz.token_sort_ratio(\"ghislain hachey\", \"hachey gislain\")) # ignores work order\n",
    "#print(fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")) # ignore duplicate words\n",
    "\n",
    "# fuzzy on  a dataset\n",
    "#choices = ['fuzzy fuzzy was a bear', 'is this a test', 'THIS IS A TEST!!']\n",
    "#process.extract(\"this is a test\", choices, scorer=fuzz.ratio)\n",
    "\n",
    "# def fuzzy_merge(df1, df2, key1, key2, threshold=90, limit=2):\n",
    "#     \"\"\"\n",
    "#     :param df1: the left table to join\n",
    "#     :param df2: the right table to join\n",
    "#     :param key1: key column of the left table\n",
    "#     :param key2: key column of the right table\n",
    "#     :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "#     :param limit: the amount of matches that will get returned, these are sorted high to low\n",
    "#     :return: dataframe with boths keys and matches\n",
    "#     \"\"\"\n",
    "#     s = df2[key2].tolist()\n",
    "\n",
    "#     m = df1[key1].apply(lambda x: process.extract(x, s, limit=limit))    \n",
    "#     df_1['matches'] = m\n",
    "\n",
    "#     m2 = df1['matches'].apply(lambda x: ', '.join([i[0] for i in x if i[1] >= threshold]))\n",
    "#     df1['matches'] = m2\n",
    "\n",
    "#     return df1\n",
    "\n",
    "#df_exams.merge(df_student_enrol, how='left', left_on='STUDENTNAME', right_on='Student', suffixes=('_from_exams', '_from_db'))\n",
    "#fuzzy_merge(df_exams, df_student_enrol, 'STUDENTNAME', 'Student', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Let's get try a bit of fuzzy search see if we can get more students matching\n",
    "# This fuzzy_pandas package is seamingly more straight forward\n",
    "import fuzzy_pandas as fpd\n",
    "\n",
    "exams_cols = list(set(df_exams.columns))\n",
    "stuen_cols = list(set(df_student_enrol.columns))\n",
    "\n",
    "df_fuzzy_matches = fpd.fuzzy_merge(\n",
    "    df_exams, df_student_enrol,\n",
    "    left_on=['STUDENTNAME'], right_on=['Student'],\n",
    "    #keep='all',\n",
    "    method='levenshtein',\n",
    "    threshold=0.9,\n",
    "    ignore_case=True,\n",
    "    ignore_nonalpha=False,\n",
    "    ignore_nonlatin=False,\n",
    "    ignore_order_words=False,\n",
    "    ignore_order_letters=False,\n",
    "    ignore_titles=False,\n",
    "    join='left-outer' # { 'inner', 'left-outer', 'right-outer', 'full-outer' }\n",
    ")\n",
    "\n",
    "df_fuzzy_matches\n",
    "\n",
    "#s = df_fuzzy_matches['stuCardID'] == ''\n",
    "#s.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repackage the data into the original format for the exams system\n",
    "df_fuzzy_matches\n",
    "\n",
    "df_fuzzy_cleaned = df_fuzzy_matches.drop(['SCHOOLID','GENDER','stueYear'] , axis='columns')\n",
    "df_fuzzy_cleaned.rename(columns={'STUDENTID':'STUDENTID_ORIG','stuCardID':'STUDENTID','stuGender':'GENDER','schNo':'SCHOOLID'}, inplace=True)\n",
    "df_fuzzy_cleaned = df_fuzzy_cleaned[[\n",
    "    'STUDENTID_ORIG', 'STUDENTID', 'SPED', 'ACCOM', 'STUDENTNAME', 'Student', 'stuDoB', 'SCHOOLID', 'GENDER', 'TESTID', 'TEACHERNAME', \n",
    "    'ITEM_001', 'ITEM_002', 'ITEM_003', 'ITEM_004',\n",
    "    'ITEM_005', 'ITEM_006', 'ITEM_007', 'ITEM_008', 'ITEM_009', 'ITEM_010',\n",
    "    'ITEM_011', 'ITEM_012', 'ITEM_013', 'ITEM_014', 'ITEM_015', 'ITEM_016',\n",
    "    'ITEM_017', 'ITEM_018', 'ITEM_019', 'ITEM_020', 'ITEM_021', 'ITEM_022',\n",
    "    'ITEM_023', 'ITEM_024', 'ITEM_025', 'ITEM_026', 'ITEM_027', 'ITEM_028',\n",
    "    'ITEM_029', 'ITEM_030', 'ITEM_031', 'ITEM_032', 'END']]\n",
    "\n",
    "# minor cleanup of teacher names: at least remove white spaces :)\n",
    "df_fuzzy_cleaned['TEACHERNAME'] = df_fuzzy_cleaned['TEACHERNAME'].str.strip()\n",
    "#df_teacher_school[df_teacher_school['TEACHERNAME'] == 'Jiem Lakmej']\n",
    "\n",
    "# Re camel case student names\n",
    "df_fuzzy_cleaned['STUDENTNAME']= df_fuzzy_cleaned['STUDENTNAME'].str.title()\n",
    "df_fuzzy_cleaned['Student']= df_fuzzy_cleaned['Student'].str.title()\n",
    "\n",
    "df_fuzzy_cleaned.sort_values(by=['STUDENTID_ORIG'])\n",
    "#df_fuzzy_cleaned.columns\n",
    "#len(df_fuzzy_cleaned['STUDENTID_ORIG'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try some educated 'guesses'. Set the school based on known variable:\n",
    "# the teachers' most commonly supervised school\n",
    "df_teacher_school = df_fuzzy_cleaned[['SCHOOLID','TEACHERNAME']].copy()\n",
    "\n",
    "# All teachers with known schools based on fuzzy search on students in EMIS data\n",
    "df_teacher_with_school = df_teacher_school[df_teacher_school['SCHOOLID'] != ''].copy()\n",
    "\n",
    "# All students remaining with no schools (or gender) known from fuzzy searching EMIS data\n",
    "df_teacher_no_school = df_teacher_school[df_teacher_school['SCHOOLID'] == ''].copy()\n",
    "\n",
    "#df_teacher_with_school#.groupby(by='TEACHERNAME')\n",
    "df_teacher_with_school_occurance = df_teacher_with_school.value_counts(sort=True, ascending=False)\n",
    "df_teacher_with_school_occurance = df_teacher_with_school_occurance.reset_index().groupby('TEACHERNAME').first()\n",
    "del df_teacher_with_school_occurance[0]\n",
    "teacher_school = df_teacher_with_school_occurance.to_dict()['SCHOOLID']\n",
    "teacher_school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by assigning a best guess school to all remaining records without one\n",
    "# However, only do this on the subset of rows with no acquired \n",
    "# information from the EMIS DB\n",
    "df_fuzzy_cleaned[['SCHOOLID','TEACHERNAME']]\n",
    "df_fuzzy_cleaned.loc[df_fuzzy_cleaned['SCHOOLID'] == '', ['SCHOOLID']] = df_fuzzy_cleaned['TEACHERNAME'].map(teacher_school)\n",
    "df_fuzzy_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write processed data back into excel for final scrutiny\n",
    "f_cleaned = os.path.join(cwd, 'data/RMI/poon-cleanup-request/M03 2019-cleaned.xlsx')\n",
    "df_fuzzy_cleaned.to_excel(f_cleaned, index=False, sheet_name='M03 2019', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
