{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# This notebook is a SOE Assessment equivalent re-engineered to have a        #\n",
    "# complete undersanding of exams data in countries using it.                  #\n",
    "###############################################################################\n",
    "# Core stuff\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Data stuff\n",
    "import pandas as pd # Data analysis\n",
    "import numpy as np\n",
    "\n",
    "# Pretty printing stuff\n",
    "from IPython.display import display, HTML\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_area_auto_adjustable\n",
    "\n",
    "# Configuration (initial setup)\n",
    "with open('config.json', 'r') as file:\n",
    "     config = json.load(file)\n",
    "\n",
    "test = config['test']\n",
    "country = config['country']\n",
    "cwd = os.getcwd()\n",
    "\n",
    "if country == 'FSM':\n",
    "    achievement_levels = ['well below competent', 'approaching competent', 'minimally competent', 'competent'] # NMCT\n",
    "elif country == 'RMI':\n",
    "    achievement_levels = ['Beginning', 'Developing', 'Proficient', 'Advanced'] # MISAT\n",
    "else:\n",
    "    achievement_levels = ['Level 1', 'Level 2', 'Level 3', 'Level 4'] # Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_to_df(filename):\n",
    "    \"\"\"Loads an Excel filename to a Pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str, required\n",
    "        The filename of the excel file to load\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    file_path = Path(filename)\n",
    "    file_extension = file_path.suffix.lower()[1:]\n",
    "\n",
    "    if file_extension == 'xlsx':\n",
    "        df_student_results = pd.read_excel(filename, index_col=None, header=0, engine='openpyxl')\n",
    "    elif file_extension == 'xls':\n",
    "        df_student_results = pd.read_excel(filename, index_col=None, header=0)\n",
    "    elif file_extension == 'csv':\n",
    "        df_student_results = pd.read_csv(filename, index_col=None, header=0)\n",
    "    else:\n",
    "        raise Exception(\"File not supported\")\n",
    "\n",
    "    return df_student_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Responses Sheet                                                             #\n",
    "###############################################################################\n",
    "\n",
    "# Load a single SOE Assessment workbook (for testing,)\n",
    "# in particular the sheet with the raw data\n",
    "cwd = os.getcwd()\n",
    "#filename = os.path.join(cwd, 'data/RMI/MISAT/MISAT 2019/3GrEng2019/AllSchools_A03_2018-19_Results.xls')\n",
    "#filename = os.path.join(cwd, 'data/RMI/MISAT/MISAT 2012/6grEng12/AllSchools_A06_2011-12_Results.xls')\n",
    "#filename = os.path.join(cwd, 'data/RMI/MISAT/MISAT 2009/3GrEng09/AllSchools_A03_2008-09_Results.xls')\n",
    "#filename = os.path.join(cwd, 'data/RMI/MISAT/MISAT 2010/6GrEng2010/AllSchools_A06_2009-10_Results.xls')\n",
    "#filename = os.path.join(cwd, 'data/RMI/MISAT/MISAT 2019/Gr3Eng2019/AllSchools_A03_2018-19_Results.xls')\n",
    "filename = os.path.join(cwd, 'data/FSM/NMCT/NMCT 2021/AllSchools_R08_2020-21_Results.xls')\n",
    "\n",
    "df_student_results = load_excel_to_df(filename)\n",
    "print('df_student_results')\n",
    "display(df_student_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###############################################################################\n",
    "# NOT NEEDED                                                                  #\n",
    "# Responses Sheet (all)                                                       #\n",
    "###############################################################################\n",
    "\n",
    "# Load all SOE Assessment workbook inside a directory\n",
    "# (~50 seconds on iMac with i9 CPU and 32GB RAM)\n",
    "#cwd = os.getcwd()\n",
    "#path = os.path.join(cwd, 'data/'+country+'/'+test+'/')\n",
    "\n",
    "#df_student_results_list = []\n",
    "\n",
    "#for root, directories, files in os.walk(path, topdown=False):\n",
    "#    for name in files:\n",
    "#        filename = os.path.join(root, name)\n",
    "#        print('Loading into DataFrame:', filename)\n",
    "#        try:\n",
    "#            df_student_results_list.append(load_excel_to_df(filename))\n",
    "#        except:\n",
    "#            print('Problem loading:', filename)\n",
    "#            #print('Error was:', )            \n",
    "\n",
    "#print('Completed loading excel files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Scores Sheet                                                                #\n",
    "###############################################################################\n",
    "def score(answer, item):\n",
    "    \"\"\" Scores the answer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    answer : String, required\n",
    "        The student's answer to an item (e.g. A, B, BLANK)\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1 is correct, 0 if incorrect\n",
    "    \"\"\"\n",
    "    if answer.upper() == item[-1].upper():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_student_results_scores = df_student_results.copy()\n",
    "\n",
    "cols = df_student_results_scores.columns.values\n",
    "cols_items = [i for i in cols if 'Item_' in i]\n",
    "cols_items.sort()\n",
    "\n",
    "for item in cols_items:\n",
    "    df_student_results_scores[item] = df_student_results_scores[item].apply(score, item, args=(item,))\n",
    "display(df_student_results_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-following",
   "metadata": {},
   "source": [
    "From Phil Geeves' NDOE NSTT Reports for computing benchmark levels for students\n",
    "\n",
    "# Appendix 1 Determining achievement levels for benchmarks\n",
    "\n",
    "The FSM curriculum is divided into subject areas, standards, year levels and benchmarks.\n",
    "The NMCT tests assesses each student as being at one of four achievement levels against\n",
    "each benchmark:\n",
    "\n",
    "- \"competent\"\n",
    "- \"minimally competent\"\n",
    "- \"approaching competent\"\n",
    "- \"well below competent\"\n",
    "\n",
    "The test is multiple choice, with, usually, four questions being used to assess the level of\n",
    "achievement of any student. To be considered competent, all four questions must be\n",
    "answered correctly.\n",
    "\n",
    "For some seven benchmarks (out of 83 in total) there are more than 4 questions relating to\n",
    "the benchmarks. These are organised in sets of 4 questions relating to a particular 'indicator'\n",
    "associated with the benchmark. Two benchmarks have 12 questions and five have 8\n",
    "questions. When determining the achievement level, the following business rules were\n",
    "used.\n",
    "\n",
    "<table>\n",
    "    <caption>Minimum number of correct answers required for each achievement level</caption>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Achievement Level</th>\n",
    "            <th colspan=\"3\">Number of questions contributing to the benchmark assessment</th>            \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>4</th>\n",
    "            <th>8</th>\n",
    "            <th>12</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>well below competent</td>\n",
    "            <td style=\"text-align: center;\">0</td>\n",
    "            <td style=\"text-align: center;\">0</td>\n",
    "            <td style=\"text-align: center;\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>approaching competent</td>\n",
    "            <td style=\"text-align: center;\">2</td>\n",
    "            <td style=\"text-align: center;\">3</td>\n",
    "            <td style=\"text-align: center;\">4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>minimally competent</td>\n",
    "            <td style=\"text-align: center;\">3</td>\n",
    "            <td style=\"text-align: center;\">5</td>\n",
    "            <td style=\"text-align: center;\">7</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>competent</td>\n",
    "            <td style=\"text-align: center;\">4</td>\n",
    "            <td style=\"text-align: center;\">7</td>\n",
    "            <td style=\"text-align: center;\">10</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# AggregateScores Sheet                                                       #\n",
    "###############################################################################\n",
    "df_student_results_aggscores = df_student_results_scores.copy()\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "###############################################################################\n",
    "\n",
    "def get_level(s):\n",
    "    \"\"\" A function to get a level (i.e. Beginning, Developing, Advanced, Proficient)\n",
    "    from a string of format A.6.2.1_L1Percent. The level is in the string itself (L1 -> Beginning).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : String, required\n",
    "        The level string representing an benchmark, standard or test (e.g. A.6.2.1_L1Percent)\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    level : String\n",
    "    \"\"\" \n",
    "    levels = {\n",
    "        'L1': 'Beginning',\n",
    "        'L2': 'Developing',\n",
    "        'L3': 'Proficient',\n",
    "        'L4': 'Advanced',\n",
    "    }\n",
    "    try: \n",
    "        level = levels[s.split('_')[1].split('Percent')[0]]\n",
    "    except:\n",
    "        level = levels[s.split('Percent')[0]]\n",
    "    return level\n",
    "\n",
    "def get_bins(total_possible_scores):\n",
    "    \"\"\" Getting total possible score is really optional code\n",
    "    since the bins are essentially 4 equal bins no matter what the total items \n",
    "    for an indicator (or benchmark, standard, test). But it is included here in case\n",
    "    one would want to adjust the width of the bins.\n",
    "    \n",
    "    Otherwise, the bins could simply be set to 4 in pandas.cut\n",
    "    and it would produce the same results as below.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    total_possible_scores : Integer, required\n",
    "        The total possible for a particular indicator or benchmark\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bins : List\n",
    "        The Scalar representing the bins\n",
    "    \"\"\" \n",
    "    \n",
    "    if len(total_possible_scores) != 1:\n",
    "        print('Something is wrong. There should not be more then one total unique scores for a particular benchmark.')\n",
    "        \n",
    "    total_possible_score = total_possible_scores[0]\n",
    "    \n",
    "    if  total_possible_scores[0] == 4:\n",
    "        bins = [0,1,2,3,4]\n",
    "    elif total_possible_scores[0] == 8:\n",
    "        bins = [0,2,4,6,8]\n",
    "    elif total_possible_scores[0] == 12:\n",
    "        bins = [0,3,6,9,12]\n",
    "    elif total_possible_scores[0] == 16:\n",
    "        bins = [0,4,8,12,16]\n",
    "    elif total_possible_scores[0] == 20:\n",
    "        bins = [0,5,10,15,20]\n",
    "    elif total_possible_scores[0] == 24:\n",
    "        bins = [0,6,12,18,24]    \n",
    "    else:\n",
    "        print('Unexpected number of bins: ', total_possible_scores)\n",
    "        # If none of the above total numner of possible score (i.e. correct items)\n",
    "        # then simply be lazy and fallback to 4 which will simply cut into \n",
    "        # 4 equal size bins (e.g. 60 items [0,15,30,45,60] which means [0-15, 16-30, 31-45, 46-60])\n",
    "        bins = 4\n",
    "        \n",
    "    return bins\n",
    "\n",
    "###############################################################################    \n",
    "# Columns e.g. A.6.2.1.3, A.6.2.1.4, A.6.2.2.1, etc. in SOE AggregateScores   \n",
    "# i.e. indicators\n",
    "###############################################################################\n",
    "\n",
    "# e.g. {'A.6.2.1.4': ['Item_001_AS0602010401E_ddd', 'Item_002_AS0602010402M_aaa',]}\n",
    "indicators_items = {}\n",
    "# e.g. {'A.6.2.1': ['Item_001_AS0602010401E_ddd', 'Item_002_AS0602010402M_aaa',]}\n",
    "benchmarks_items = {}\n",
    "# e.g. {'A.6.2': ['Item_001_AS0602010401E_ddd', 'Item_002_AS0602010402M_aaa',]}\n",
    "standards_items = {}\n",
    "# e.g. {'A.6': ['Item_001_AS0602010401E_ddd', 'Item_002_AS0602010402M_aaa',]}\n",
    "test_items = {}\n",
    "\n",
    "def compile_items(item):\n",
    "    \"\"\" A function to compile the related items into their indicators (e.g. Test.Grade.Standard.Benchmark.Indicator), benchmarks,\n",
    "    standards and test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    item : String, required\n",
    "        The item string (e.g. Item_002_AS0602010402M_aaa)\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Nothing\n",
    "    \"\"\"\n",
    "    item_meta = item.split('_')\n",
    "    item_parts = list(item_meta[2])\n",
    "    indicator = item_parts[0] + '.' + item_parts[3] + '.' + item_parts[5] + '.' + item_parts[7] + '.' + item_parts[9]\n",
    "    benchmark = item_parts[0] + '.' + item_parts[3] + '.' + item_parts[5] + '.' + item_parts[7]\n",
    "    standard = item_parts[0] + '.' + item_parts[3] + '.' + item_parts[5]\n",
    "    test = item_parts[0] + '.' + item_parts[3]\n",
    "    \n",
    "    # Check if indicator already added, if not add it\n",
    "    if indicator in indicators_items:       \n",
    "        indicators_items[indicator].append(item)\n",
    "    else:\n",
    "        indicators_items[indicator] = [item]\n",
    "        \n",
    "    # Check if benchmark already added, if not add it\n",
    "    if benchmark in benchmarks_items:       \n",
    "        benchmarks_items[benchmark].append(item)\n",
    "    else:\n",
    "        benchmarks_items[benchmark] = [item]\n",
    "    \n",
    "    # Check if standard already added, if not add it\n",
    "    if standard in standards_items:       \n",
    "        standards_items[standard].append(item)\n",
    "    else:\n",
    "        standards_items[standard] = [item]\n",
    "        \n",
    "    # Check if test already added, if not add it\n",
    "    if test in test_items:       \n",
    "        test_items[test].append(item)\n",
    "    else:\n",
    "        test_items[test] = [item]    \n",
    "\n",
    "cols = df_student_results_aggscores.columns.values\n",
    "cols_items = [i for i in cols if 'Item_' in i]\n",
    "\n",
    "for i in cols_items:\n",
    "    compile_items(i)\n",
    "\n",
    "for ind in sorted(indicators_items.keys()):\n",
    "    items = indicators_items[ind]\n",
    "    df_student_results_aggscores[ind] = df_student_results_aggscores.loc[:, items].sum(axis=1)    \n",
    "\n",
    "###############################################################################    \n",
    "# Columns e.g. A.6.2.1.3Total, A.6.2.1.4Total, A.6.2.2.1Total, not shown in SOE AggregateScores\n",
    "# but useful in calculation later on\n",
    "###############################################################################\n",
    "for ind in sorted(indicators_items.keys()):\n",
    "    items = indicators_items[ind]\n",
    "    df_student_results_aggscores[ind+'Total'] = df_student_results_aggscores.loc[:, items].count(axis=1)\n",
    "    \n",
    "###############################################################################    \n",
    "# Columns e.g. A.6.2.1, A.6.2.2, etc. not in SOE AggregateScores\n",
    "# i.e. benchmarks (to bypass indicator like Phill Geeves)\n",
    "###############################################################################    \n",
    "for ben in sorted(benchmarks_items.keys()):\n",
    "    items = benchmarks_items[ben]\n",
    "    df_student_results_aggscores[ben] = df_student_results_aggscores.loc[:, items].sum(axis=1)    \n",
    "\n",
    "###############################################################################    \n",
    "# Columns e.g. A.6.2.1Total, A.6.2.2Total, not shown in SOE AggregateScores\n",
    "# but useful in calculation later on (to bypass indicator like Phill Geeves)\n",
    "###############################################################################\n",
    "for ben in sorted(benchmarks_items.keys()):\n",
    "    items = benchmarks_items[ben]\n",
    "    df_student_results_aggscores[ben+'Total'] = df_student_results_aggscores.loc[:, items].count(axis=1)\n",
    "    \n",
    "###############################################################################    \n",
    "# Columns e.g. A.6.2, etc. not in SOE AggregateScores   \n",
    "# i.e. standards (to bypass indicator like Phill Geeves)\n",
    "###############################################################################    \n",
    "for sta in sorted(standards_items.keys()):\n",
    "    items = standards_items[sta]\n",
    "    df_student_results_aggscores[sta] = df_student_results_aggscores.loc[:, items].sum(axis=1)    \n",
    "\n",
    "###############################################################################    \n",
    "# Columns e.g. A.6.2Total, etc. not shown in SOE AggregateScores\n",
    "# but useful in calculations later on (to bypass indicator like Phill Geeves)\n",
    "###############################################################################\n",
    "for sta in sorted(standards_items.keys()):\n",
    "    items = standards_items[sta]\n",
    "    df_student_results_aggscores[sta+'Total'] = df_student_results_aggscores.loc[:, items].count(axis=1)\n",
    "    \n",
    "###############################################################################    \n",
    "# Columns e.g. A.6, etc. not in SOE AggregateScores   \n",
    "# i.e. test (to bypass indicator like Phill Geeves)\n",
    "###############################################################################    \n",
    "for tes in sorted(test_items.keys()):\n",
    "    items = test_items[tes]\n",
    "    df_student_results_aggscores[tes] = df_student_results_aggscores.loc[:, items].sum(axis=1)    \n",
    "\n",
    "###############################################################################    \n",
    "# Columns e.g. A.6Total, etc. not shown in SOE AggregateScores\n",
    "# but useful in calculations later on (to bypass indicator like Phill Geeves)\n",
    "###############################################################################\n",
    "for tes in sorted(test_items.keys()):\n",
    "    items = test_items[tes]\n",
    "    df_student_results_aggscores[tes+'Total'] = df_student_results_aggscores.loc[:, items].count(axis=1)\n",
    "    \n",
    "###############################################################################\n",
    "# Columns e.g. A.6.2.1.3Level, A.6.2.1.4Level, A.6.2.2.1Level, etc. in SOE AggregateScores\n",
    "#\n",
    "# Business rule: \n",
    "# Essentially standard bins technique where total items correct from total items will\n",
    "# define the level. Results do vary when items are not a multiple of 4 for a given\n",
    "# indicator (a bit rare but to note)\n",
    "###############################################################################\n",
    "for ind in sorted(indicators_items.keys()):\n",
    "    items = indicators_items[ind]\n",
    "    total_possible_scores = df_student_results_aggscores[ind+'Total'].unique()\n",
    "    bins = get_bins(total_possible_scores)\n",
    "        \n",
    "    df_student_results_aggscores[ind+'Level'] = pd.cut(df_student_results_aggscores[ind], bins, \n",
    "                                                       labels=achievement_levels, include_lowest=True)\n",
    "    \n",
    "###############################################################################    \n",
    "# Columns e.g. A.6.2.1_L1Percent, A.6.2.1_L2Percent, A.6.2.1_L3Percent, A.6.2.1_L4Percent, A.6.2.2_L1Percent, A.6.2.2_L2Percent, etc. in SOE AggregateScores\n",
    "# i.e. benchmarks weighted scores\n",
    "###############################################################################\n",
    "# e.g. {'A.6.2.1': ['A.6.2.1.3Level', 'A.6.2.1.4Level',]}\n",
    "benchmarks_indicators_levels = {}\n",
    "# e.g. {'A.6.2.1': ['A.6.2.1.3', 'A.6.2.1.4',]}\n",
    "benchmarks_indicators = {}\n",
    "# e.g. {'A.6.2.1': ['A.6.2.1_L1Percent', 'A.6.2.1_L2Percent', 'A.6.2.1_L3Percent', 'A.6.2.1_L4Percent']}\n",
    "benchmarks_levels_percent = {}\n",
    "\n",
    "def compile_benchmarks(level):\n",
    "    \"\"\" A function to compile the related indicators into their benchmarks (e.g. Test.Grade.Standard.Benchmark).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    level : String, required\n",
    "        The level string representing an indicator (e.g. A.6.2.1.3Level)\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Nothing\n",
    "    \"\"\"    \n",
    "    level_parts = level.split('.')\n",
    "    benchmark = level_parts[0] + '.' + level_parts[1] + '.' + level_parts[2] + '.' + level_parts[3]\n",
    "    #print('benchmark:', benchmark)\n",
    "    # Check if group already added, if not add it\n",
    "    indicator = level.split('Level')[0]\n",
    "    if benchmark in benchmarks_indicators_levels:       \n",
    "        benchmarks_indicators_levels[benchmark].append(level)\n",
    "        benchmarks_indicators[benchmark].append(indicator)        \n",
    "    else:\n",
    "        benchmarks_indicators_levels[benchmark] = [level]\n",
    "        benchmarks_indicators[benchmark] = [indicator]\n",
    "        benchmarks_levels_percent[benchmark] = [benchmark+'_L1Percent',benchmark+'_L2Percent',benchmark+'_L3Percent',benchmark+'_L4Percent']\n",
    "\n",
    "# Get indicators Level columns (i.e. A.6.2.1.3Level, A.6.2.1.4Level, A.6.2.2.1Level, etc.)\n",
    "cols = df_student_results_aggscores.columns.values\n",
    "cols_indicators_levels = [i for i in cols if 'Level' in i]        \n",
    "\n",
    "for i in cols_indicators_levels:\n",
    "    compile_benchmarks(i)\n",
    "\n",
    "for b in sorted(benchmarks_indicators_levels.keys()):\n",
    "    # Total indicators for the benchmark\n",
    "    total_indicators = len(benchmarks_indicators_levels[b])\n",
    "    print('A total of {} indicators ({}) for benchmarks {}.'.format(total_indicators, benchmarks_indicators_levels[b], b))  \n",
    "    #level_nums = [i+'Num' for i in benchmarks_indicators_levels[b]]\n",
    "    #print('level_nums', level_nums)\n",
    "    \n",
    "    df_level = df_student_results_aggscores.loc[:, benchmarks_indicators_levels[b]]    \n",
    "    df_student_results_aggscores[b+'_L1Percent'] = df_level[ df_level == 'Beginning' ].count(axis='columns') / total_indicators\n",
    "    df_student_results_aggscores[b+'_L2Percent'] = df_level[ df_level == 'Developing' ].count(axis='columns') / total_indicators\n",
    "    df_student_results_aggscores[b+'_L3Percent'] = df_level[ df_level == 'Proficient' ].count(axis='columns') / total_indicators\n",
    "    df_student_results_aggscores[b+'_L4Percent'] = df_level[ df_level == 'Advanced' ].count(axis='columns') / total_indicators\n",
    "\n",
    "###############################################################################    \n",
    "# Columns e.g. A.6.2.1Level, A.6.2.2Level, etc. not in SOE AggregateScores\n",
    "# but used in analyzing benchmarks following the student count by levels analysis (not level count)\n",
    "# This approach actually builds on what it seems like SOE was heading for with his\n",
    "# *_L1Percent, *_L2Percent, *_L3Percent, *_L4Percent columns (totalling 1). \n",
    "# But SOE does not seem to use this in his results anaylis.\n",
    "# Also referred in Pacific EMIS as \"weighted scores\"\n",
    "#\n",
    "# This technique still requires to go through indicators \"in the background\"\n",
    "#\n",
    "# This will need to be set on new defined business rule. They are based (calculated on)\n",
    "# the columns e.g. A.6.2.1_L1Percent, A.6.2.1_L2Percent, A.6.2.1_L3Percent, A.6.2.1_L4Percent, A.6.2.2_L1Percent, A.6.2.2_L2Percent, A.6.2.2_L3Percent, A.6.2.2_L4Percent (i.e. benchmarks)\n",
    "# The level with the highest percentage can be used. If two or more levels have equal percentages\n",
    "# then take the (best or worst level?). It can do both by commenting/uncommenting lines below\n",
    "# i.e. benchmarks\n",
    "###############################################################################\n",
    "\n",
    "for b in benchmarks_levels_percent:\n",
    "    df1 = df_student_results_aggscores[benchmarks_levels_percent[b]] #.copy()\n",
    "    # START: If highest maximum level is sought\n",
    "    cols = df1.columns.to_list()\n",
    "    cols.sort(reverse=True)\n",
    "    df1 = df1[cols]\n",
    "    # END: If highest maximum level is sought\n",
    "    df_student_results_aggscores[b+'Level'] = df1.idxmax(axis=1)\n",
    "    df_student_results_aggscores[b+'Level'] = df_student_results_aggscores[b+'Level'].apply(lambda x: get_level(x))\n",
    "\n",
    "\n",
    "    \n",
    "###############################################################################    \n",
    "# Columns e.g. A.6.2_L1Percent, A.6.2_L2Percent, A.6.2_L3Percent, A.6.2_L4Percent, etc. in SOE AggregateScores\n",
    "# i.e. standards\n",
    "###############################################################################\n",
    "\n",
    "# e.g. {'A.6.2': ['A.6.2.1.3Level', 'A.6.2.1.4Level', 'A.6.2.2.1Level', etc.]}\n",
    "standards_indicators_levels = {}\n",
    "# e.g. {'A.6.2': ['A.6.2.1.3', 'A.6.2.1.4', 'A.6.2.2.1', etc.]}\n",
    "standards_indicators = {}\n",
    "# e.g. {'A.6.2': ['A.6.2_L1Percent','A.6.2_L2Percent','A.6.2_L3Percent','A.6.2_L4Percent']}\n",
    "standards_levels_percent = {}\n",
    "\n",
    "def compile_standards(level):\n",
    "    \"\"\" A function to compile the related indicators into their standards (e.g. Test.Grade.Standard).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    level : String, required\n",
    "        The level string representing an indicator (e.g. A.6.2.1.3Level)\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Nothing\n",
    "    \"\"\"    \n",
    "    level_parts = level.split('.')\n",
    "    standard = level_parts[0] + '.' + level_parts[1] + '.' + level_parts[2]\n",
    "    #print('standard:', standard)\n",
    "    # Check if standard already added, if not add it\n",
    "    indicator = level.split('Level')[0]\n",
    "    if standard in standards_indicators_levels:       \n",
    "        standards_indicators_levels[standard].append(level)\n",
    "        standards_indicators[standard].append(indicator)\n",
    "    else:\n",
    "        standards_indicators_levels[standard] = [level]\n",
    "        standards_indicators[standard] = [indicator]\n",
    "        standards_levels_percent[standard] = [standard+'_L1Percent',standard+'_L2Percent',standard+'_L3Percent',standard+'_L4Percent']\n",
    "\n",
    "# Get Level benchmarks columns (i.e. A.6.2.1Level, A.6.2.2Level, etc.)\n",
    "# At this point we now have additional *Level columns for benchmarks\n",
    "cols = df_student_results_aggscores.columns.values\n",
    "cols_benchmarks_levels = [i for i in cols if 'Level' in i] \n",
    "cols_benchmarks_levels = list(set(cols_benchmarks_levels) - set(cols_indicators_levels))\n",
    "\n",
    "for i in cols_indicators_levels:\n",
    "    compile_standards(i)\n",
    "    \n",
    "for s in sorted(standards_indicators_levels.keys()):\n",
    "    # Total indicators for the standard\n",
    "    total_indicators = len(standards_indicators_levels[s])\n",
    "    print('A total of {} indicators ({}) for standards {}.'.format(total_indicators, standards_indicators_levels[s], s))  \n",
    "    \n",
    "    df_level = df_student_results_aggscores.loc[:, standards_indicators_levels[s]]\n",
    "    df_student_results_aggscores[s+'_L1Percent'] = df_level[ df_level == 'Beginning' ].count(axis='columns') / total_indicators\n",
    "    df_student_results_aggscores[s+'_L2Percent'] = df_level[ df_level == 'Developing' ].count(axis='columns') / total_indicators\n",
    "    df_student_results_aggscores[s+'_L3Percent'] = df_level[ df_level == 'Proficient' ].count(axis='columns') / total_indicators\n",
    "    df_student_results_aggscores[s+'_L4Percent'] = df_level[ df_level == 'Advanced' ].count(axis='columns') / total_indicators    \n",
    "\n",
    "###############################################################################    \n",
    "# Columns e.g. A.6.2Level, etc. not in SOE AggregateScores\n",
    "# but used in analyzing standards following the student count by levels analysis (not level count)\n",
    "# This approach actually builds on what it seems like SOE was heading for with his\n",
    "# *_L1Percent, *_L2Percent, *_L3Percent, *_L4Percent columns (totalling 1). \n",
    "# But SOE does not seem to use this in his results analysis.\n",
    "# Also referred in Pacific EMIS as \"weighted scores\"\n",
    "#\n",
    "# This technique still requires to go through indicators \"in the background\"\n",
    "# \n",
    "# This will need to be set on defined business rule. They are based (calculated on)\n",
    "# the columns e.g. A.6.2_L1Percent, A.6.2_L2Percent, A.6.2_L3Percent, A.6.2_L4Percent (i.e. standards)\n",
    "# The level with the higher percentage can be used. If two or more levels have equal percentages\n",
    "# then take the (best or worst level?)\n",
    "# i.e. standards\n",
    "###############################################################################\n",
    "\n",
    "for s in standards_levels_percent:\n",
    "    df1 = df_student_results_aggscores[standards_levels_percent[s]] #.copy()\n",
    "    # START: If highest maximum level is sought\n",
    "    cols = df1.columns.to_list()\n",
    "    cols.sort(reverse=True)\n",
    "    df1 = df1[cols]\n",
    "    # END: If highest maximum level is sought\n",
    "    df_student_results_aggscores[s+'Level'] = df1.idxmax(axis=1)\n",
    "    df_student_results_aggscores[s+'Level'] = df_student_results_aggscores[s+'Level'].apply(lambda x: get_level(x))\n",
    "\n",
    "###############################################################################  \n",
    "# Column TotalScore_* in SOE AggregateScores   \n",
    "###############################################################################  \n",
    "df_student_results_aggscores['TotalScore'] = df_student_results_aggscores.loc[:, cols_items].sum(axis=1)\n",
    "df_student_results_aggscores['TotalScore_LowerLimit'] = df_student_results_aggscores['TotalScore'] - 6\n",
    "df_student_results_aggscores['TotalScore_UpperLimit'] = df_student_results_aggscores['TotalScore'] + 6\n",
    "\n",
    "###############################################################################    \n",
    "# Columns e.g. L1Percent, L2Percent, L3Percent, L4Percent, etc. in SOE AggregateScores\n",
    "# Should be named A.6L1Percent, A.6L2Percent, A.6L3Percent, A.6L4Percent, etc. for consistency\n",
    "###############################################################################\n",
    "\n",
    "# e.g. {'A.6': ['A.6.2.1.3Level', 'A.6.2.1.4Level', 'A.6.2.2.1Level', etc.]}\n",
    "test_indicators_levels = {}\n",
    "# e.g. {'A.6': ['A.6.2.1.3', 'A.6.2.1.4', 'A.6.2.2.1', etc.]}\n",
    "test_indicators = {}\n",
    "# e.g. {'A.6': ['L1Percent','L2Percent','L3Percent','L4Percent']}\n",
    "# or if not following Dr. SOE to be more consistent would have been {'A.6': ['A.6_L1Percent','A.6_L2Percent','A.6_L3Percent','A.6_L4Percent']}\n",
    "test_levels_percent = {}\n",
    "\n",
    "\n",
    "def compile_test(level):\n",
    "    \"\"\" A function to compile the related indicators into the whole test (e.g. Test.Grade).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    level : String, required\n",
    "        The level string representing an indicator (e.g. A.6.2.1.3Level)\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Nothing\n",
    "    \"\"\"    \n",
    "    level_parts = level.split('.')\n",
    "    test = level_parts[0] + '.' + level_parts[1]\n",
    "    #print('test:', test)\n",
    "    # Check if test already added, if not add it\n",
    "    indicator = level.split('Level')[0]\n",
    "    if test in test_indicators_levels:       \n",
    "        test_indicators_levels[test].append(level)\n",
    "        test_indicators[test].append(indicator)\n",
    "    else:        \n",
    "        test_indicators_levels[test] = [level]\n",
    "        test_indicators[test] = [indicator]\n",
    "        test_levels_percent[test] = ['L1Percent','L2Percent','L3Percent','L4Percent']\n",
    "\n",
    "# Get Level standards columns (i.e. A.6.2Level, etc.)\n",
    "# At this point we now have additional *Level columns for standards and benchmarks\n",
    "cols = df_student_results_aggscores.columns.values\n",
    "cols_standards_levels = [i for i in cols if 'Level' in i] \n",
    "cols_standards_levels = list(set(cols_standards_levels) - set(cols_benchmarks_levels) - set(cols_indicators_levels))\n",
    "        \n",
    "for i in cols_indicators_levels:\n",
    "    compile_test(i)\n",
    "    \n",
    "for t in sorted(test_indicators_levels.keys()):\n",
    "    # Total indicators for the test\n",
    "    total_indicators = len(test_indicators_levels[t])\n",
    "    print('A total of {} indicators ({}) for test {}.'.format(total_indicators, test_indicators_levels[t], t))  \n",
    "    \n",
    "    df_level = df_student_results_aggscores.loc[:, test_indicators_levels[t]]\n",
    "    df_student_results_aggscores['L1Percent'] = df_level[ df_level == 'Beginning' ].count(axis='columns') / total_indicators\n",
    "    df_student_results_aggscores['L2Percent'] = df_level[ df_level == 'Developing' ].count(axis='columns') / total_indicators\n",
    "    df_student_results_aggscores['L3Percent'] = df_level[ df_level == 'Proficient' ].count(axis='columns') / total_indicators\n",
    "    df_student_results_aggscores['L4Percent'] = df_level[ df_level == 'Advanced' ].count(axis='columns') / total_indicators\n",
    "\n",
    "###############################################################################    \n",
    "# Columns e.g. A.6Level not in SOE AggregateScores\n",
    "# but used in analyzing standards following the student count by levels analysis (not level count)\n",
    "# This approach actually builds on what it seems like SOE was heading for with his\n",
    "# *_L1Percent, *_L2Percent, *_L3Percent, *_L4Percent columns (totalling 1). \n",
    "# But SOE does not seem to use this in his results analysis.\n",
    "# Also referred in Pacific EMIS as \"weighted scores\"\n",
    "#\n",
    "# This technique still requires to go through indicators \"in the background\"\n",
    "#\n",
    "# This will need to be set on defined business rule. They are based (calculated on)\n",
    "# the columns e.g. L1Percent, L2Percent, L3Percent, L4Percent (i.e. test)\n",
    "# The level with the higher percentage can be used. If two or more levels have equal percentages\n",
    "# then take the (best or worst level?)\n",
    "# i.e. test\n",
    "###############################################################################\n",
    "\n",
    "for t in test_levels_percent:\n",
    "    df1 = df_student_results_aggscores[test_levels_percent[t]] #.copy()\n",
    "    # START: If highest maximum level is sought\n",
    "    cols = df1.columns.to_list()\n",
    "    cols.sort(reverse=True)\n",
    "    df1 = df1[cols]\n",
    "    # END: If highest maximum level is sought\n",
    "    df_student_results_aggscores[t+'Level'] = df1.idxmax(axis=1)\n",
    "    df_student_results_aggscores[t+'Level'] = df_student_results_aggscores[t+'Level'].apply(lambda x: get_level(x))\n",
    "    \n",
    "###############################################################################        \n",
    "# Column AYP (Level 3 and 4) in SOE AggregateScores   \n",
    "###############################################################################    \n",
    "df_student_results_aggscores['AYP'] = df_student_results_aggscores['L3Percent'] + df_student_results_aggscores['L4Percent']\n",
    "\n",
    "# Final column cleanup    \n",
    "df_student_results_aggscores = df_student_results_aggscores.drop(cols_items, 1)\n",
    "\n",
    "# Get Level test columns (i.e. A.6Level)\n",
    "# At this point we now have additional *Level columns for test, standards and benchmarks\n",
    "cols = df_student_results_aggscores.columns.values\n",
    "cols_test_levels = [i for i in cols if 'Level' in i] \n",
    "cols_test_levels = list(set(cols_test_levels) - set(cols_standards_levels) - set(cols_benchmarks_levels) - set(cols_indicators_levels))\n",
    "\n",
    "###############################################################################\n",
    "# The following offers an alternative way of producing analysis on benchmarks,\n",
    "# standards and test based directy on their respective items (not so called level count\n",
    "# as in SOE).\n",
    "# This is more akin to how \"indicator\" analysis in SOE works. \n",
    "# This is *not* in SOE AggregateScores.\n",
    "# This is more based on Phill Geeves final report (refer to Appendix 1 in cell above)\n",
    "# and also how other common assessment system works (e.g. OnlineSBA by Pacific Testing)\n",
    "#\n",
    "# Business rule:\n",
    "# Use the ItemCount method exactly as we do to calculate the Candidate/Indicator Level. \n",
    "# Specifically, take the sum across all Items that contribute to all Indicators in the Benchmark (or Standard, Whole Test)\n",
    "# (in other words, all the benchmarks' respective items), and convert the ratio of Correct Items / Total Items \n",
    "# back to an achievement Level 1-4 (bins)\n",
    "#\n",
    "# Compare results with above for curiosity\n",
    "###############################################################################\n",
    "for ben in sorted(benchmarks_items.keys()):\n",
    "    items = benchmarks_items[ben]\n",
    "    total_possible_scores = df_student_results_aggscores[ben+'Total'].unique()\n",
    "    bins = get_bins(total_possible_scores)\n",
    "    df_student_results_aggscores[ben+'LevelAlt'] = pd.cut(df_student_results_aggscores[ben], bins, \n",
    "                                                       labels=achievement_levels, include_lowest=True)\n",
    "for sta in sorted(standards_items.keys()):\n",
    "    items = standards_items[sta]\n",
    "    total_possible_scores = df_student_results_aggscores[sta+'Total'].unique()\n",
    "    bins = get_bins(total_possible_scores)\n",
    "    df_student_results_aggscores[sta+'LevelAlt'] = pd.cut(df_student_results_aggscores[sta], bins, \n",
    "                                                       labels=achievement_levels, include_lowest=True)\n",
    "for tes in sorted(test_items.keys()):\n",
    "    items = test_items[tes]\n",
    "    total_possible_scores = df_student_results_aggscores[tes+'Total'].unique()\n",
    "    bins = get_bins(total_possible_scores)\n",
    "    df_student_results_aggscores[tes+'LevelAlt'] = pd.cut(df_student_results_aggscores[tes], bins, \n",
    "                                                       labels=achievement_levels, include_lowest=True)    \n",
    "\n",
    "# Get Level benchmarks columns (i.e. A.6.2.1LevelAlt, A.6.2.2LevelAlt, etc.)\n",
    "# At this point we now have additional *LevelAlt columns for benchmarks, standards and test\n",
    "cols = df_student_results_aggscores.columns.values\n",
    "cols_levels_alt = [i for i in cols if 'LevelAlt' in i]\n",
    "\n",
    "cols_benchmarks_levels_alt = []\n",
    "cols_standards_levels_alt = []\n",
    "cols_test_levels_alt = []\n",
    "\n",
    "for la in cols_levels_alt:\n",
    "    parts = len(la.split(\".\"))\n",
    "    #print(\"Part\", parts)\n",
    "    if parts == 4:\n",
    "        cols_benchmarks_levels_alt.append(la)\n",
    "    elif parts == 3:\n",
    "        cols_standards_levels_alt.append(la)\n",
    "    elif parts == 2:\n",
    "        cols_test_levels_alt.append(la)\n",
    "    else:\n",
    "        print(\"Error this test does not seem like others. Check the naming convention of Test.Standard.Benchmark.Indicator\", la)\n",
    "        \n",
    "\n",
    "print('indicators_items')\n",
    "pp.pprint(indicators_items)\n",
    "print('benchmarks_items')\n",
    "pp.pprint(benchmarks_items)\n",
    "print('standards_items')\n",
    "pp.pprint(standards_items)\n",
    "print('test_items')\n",
    "pp.pprint(test_items)\n",
    "\n",
    "print('cols_indicators_levels')\n",
    "pp.pprint(cols_indicators_levels)\n",
    "print('cols_benchmarks_levels')\n",
    "pp.pprint(cols_benchmarks_levels)\n",
    "print('cols_standards_levels')\n",
    "pp.pprint(cols_standards_levels)\n",
    "print('cols_test_levels')\n",
    "pp.pprint(cols_test_levels)\n",
    "\n",
    "print('benchmarks_indicators_levels')\n",
    "pp.pprint(benchmarks_indicators_levels)\n",
    "print('benchmarks_indicators')\n",
    "pp.pprint(benchmarks_indicators)\n",
    "print('benchmarks_levels_percent')\n",
    "pp.pprint(benchmarks_levels_percent)\n",
    "print('standards_indicators_levels')\n",
    "pp.pprint(standards_indicators_levels)\n",
    "print('standards_indicators')\n",
    "pp.pprint(standards_indicators)\n",
    "print('standards_levels_percent')\n",
    "pp.pprint(standards_levels_percent)\n",
    "print('test_indicators_levels')\n",
    "pp.pprint(test_indicators_levels)\n",
    "print('test_indicators')\n",
    "pp.pprint(test_indicators)\n",
    "print('test_levels_percent')\n",
    "pp.pprint(test_levels_percent)\n",
    "\n",
    "print('cols_benchmarks_levels_alt')\n",
    "pp.pprint(cols_benchmarks_levels_alt)\n",
    "print('cols_standards_levels_alt')\n",
    "pp.pprint(cols_standards_levels_alt)\n",
    "print('cols_test_levels_alt')\n",
    "pp.pprint(cols_test_levels_alt)\n",
    "\n",
    "#print(df_student_results_aggscores.columns)\n",
    "#display(df_student_results_aggscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(benchmarks_levels_percent.values())\n",
    "benchmarks_levels_percent_flattened = [val for sublist in list(benchmarks_levels_percent.values()) for val in sublist]\n",
    "benchmarks_levels_percent_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Results Sheet                                                               #\n",
    "###############################################################################\n",
    "df_student_results_analysis = df_student_results_aggscores.copy()\n",
    "#display(df_student_results_analysis)\n",
    "\n",
    "# Flatten all the levels_percent lists for use later in the weighted technique\n",
    "benchmarks_levels_percent_flattened = [val for sublist in list(benchmarks_levels_percent.values()) for val in sublist]\n",
    "standards_levels_percent_flattened = [val for sublist in list(standards_levels_percent.values()) for val in sublist]\n",
    "test_levels_percent_flattened = [val for sublist in list(test_levels_percent.values()) for val in sublist]\n",
    "\n",
    "\n",
    "print('School Name = {} (N = {})'.format('AllSchools', df_student_results_analysis.count()[0]))\n",
    "print('Test Name = {} (Test Date = {})'.format(df_student_results_analysis['TestName'][0], df_student_results_analysis['SchoolYear'][0]))\n",
    "\n",
    "df_indicators = df_student_results_analysis[['StudentName','Gender'] + cols_indicators_levels] #['StudentName'] + \n",
    "print('Indicators Levels')\n",
    "display(df_indicators)\n",
    "\n",
    "df_benchmarks = df_student_results_analysis[['StudentName','Gender'] + cols_benchmarks_levels] #['StudentName'] + \n",
    "print('Benchmarks Levels')\n",
    "display(df_benchmarks)\n",
    "\n",
    "df_benchmarks_alt = df_student_results_analysis[['StudentName','Gender'] + cols_benchmarks_levels_alt] #['StudentName'] + \n",
    "print('Benchmarks Levels Alt')\n",
    "display(df_benchmarks_alt)\n",
    "\n",
    "df_benchmarks_weighted = df_student_results_analysis[['StudentName','Gender'] + cols_benchmarks_levels_alt + benchmarks_levels_percent_flattened] #['StudentName'] + \n",
    "print('Benchmarks Levels Weighted')\n",
    "display(df_benchmarks_weighted)\n",
    "\n",
    "df_standards = df_student_results_analysis[['StudentName','Gender'] + cols_standards_levels] #['StudentName'] + \n",
    "print('Standards Levels')\n",
    "display(df_standards)\n",
    "\n",
    "df_standards_alt = df_student_results_analysis[['StudentName','Gender'] + cols_standards_levels_alt] #['StudentName'] + \n",
    "print('Standards Levels Alt')\n",
    "display(df_standards_alt)\n",
    "\n",
    "df_standards_weighted = df_student_results_analysis[['StudentName','Gender'] + cols_standards_levels_alt + standards_levels_percent_flattened] #['StudentName'] + \n",
    "print('Standards Levels Weighted')\n",
    "display(df_standards_weighted)\n",
    "\n",
    "df_test = df_student_results_analysis[['StudentName','Gender'] + cols_test_levels] #['StudentName'] + \n",
    "print('Test Levels')\n",
    "display(df_test)\n",
    "\n",
    "df_test_alt = df_student_results_analysis[['StudentName','Gender'] + cols_test_levels_alt] #['StudentName'] + \n",
    "print('Test Levels Alt')\n",
    "display(df_test_alt)\n",
    "\n",
    "df_test_weighted = df_student_results_analysis[['StudentName','Gender'] + cols_test_levels_alt + test_levels_percent_flattened] #['StudentName'] + \n",
    "print('Test Levels Weighted')\n",
    "display(df_test_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for cells that follow\n",
    "\n",
    "def prepare_for_chart(df):\n",
    "    \"\"\"Does some basic redundent preparation to a dataframe before plotting with matplotlib.\n",
    "    Essentially it does the following:\n",
    "     * Computes the percentage (e.g. 0.1, 0.9)\n",
    "     * Adds a Total row with 100 percent (i.e. 1)\n",
    "     * Rounds all values to 2 decimals\n",
    "     * Re-order the levels ready for plotting\n",
    "     * Assign negative values for levels to be on the bottom (or left) of the axis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : df, required\n",
    "        The DataFrame to prep\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    # When level values don't add up to 1 it's because of rounding\n",
    "    df = df.apply(lambda x: x / float(x.sum()))\n",
    "    df.loc['Total'] = df.sum()\n",
    "    df = df.round(2)\n",
    "    levels_index = ['Proficient','Advanced','Developing','Beginning','Total']\n",
    "    df = df.reindex(levels_index)\n",
    "    df.loc[['Developing','Beginning']] = df.loc[['Developing','Beginning']].apply(lambda x: -x)\n",
    "    return df\n",
    "\n",
    "def add_total_in_column_names(df, index='Index'):\n",
    "    \"\"\"Adds a string or the form (n=X) in the columns showing the total.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame, required\n",
    "        The DataFrame to prep\n",
    "    cos_levels : List, OBSOLETE\n",
    "        A list of levels columns\n",
    "    index : String, required\n",
    "        Whether we dealing with Index or MultiIndex\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    if index == 'Index':\n",
    "        #print('Dealing with a pd.core.indexes.base.Index')\n",
    "        #for l in cols_levels:\n",
    "        #    tot = df[l.split('Level')[0]].sum()\n",
    "        #    df = df.rename(columns = {l.split('Level')[0]: l.split('Level')[0]+' (n='+str(tot)+')'})\n",
    "        for c in df.columns:\n",
    "            tot = df[c].sum()\n",
    "            df = df.rename(columns = {c: c+' (n='+str(tot)+')'})\n",
    "        return df\n",
    "    elif index == 'MultiIndex':\n",
    "        #print('Dealing with a pd.core.indexes.multi.MultiIndex')\n",
    "        # First flatten MultiIndex (Indicator/Gender)\n",
    "        df.columns = ['_'.join(col) for col in df.columns.values]\n",
    "\n",
    "        #print(\"Troubleshooting MultiIndex\")\n",
    "        #display(df)\n",
    "        #print(df.columns)\n",
    "        #for l in cols_levels:\n",
    "        #    for g in ['f','m']:\n",
    "        #        col = l.split('Level')[0]+'_'+g\n",
    "        #        tot = df[col].sum()\n",
    "        #        df = df.rename(columns = {col: col+' (n='+str(tot)+')'})\n",
    "        for c in df.columns:\n",
    "            tot = df[c].sum()\n",
    "            df = df.rename(columns = {c: c+' (n='+str(tot)+')'})        \n",
    "\n",
    "        # Unflatten back to MultiIndex\n",
    "        cols = [col.split('_') for col in df.columns.values]\n",
    "        arrays = [[ i for i, j in cols ], [ j for i, j in cols ]]\n",
    "        df.columns = pd.MultiIndex.from_arrays(arrays, names=(None, 'Gender'))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Results Sheet                                                               \n",
    "# Analysis by indicators just like SOE assessment                             \n",
    "# BUT also provides:\n",
    "#  * gender disaggregation not provided in SOE at this level \n",
    "#  * with totals\n",
    "#  * extended versions (with descriptions)\n",
    "#  * benchmarks, standards and test analysed just like indicators \n",
    "#    (student count)\n",
    "###############################################################################\n",
    "\n",
    "descriptions = {\n",
    "    'indicators' : {\n",
    "        'A.6.2.1.3': 'A.6.2.1.3 - And some description of an indicator, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.1.4': 'A.6.2.1.4 - And some description of an indicator, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.2.1': 'A.6.2.2.1 - And some description of an indicator, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.2.2': 'A.6.2.2.2 - And some description of an indicator, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.2.4': 'A.6.2.2.4 - And some description of an indicator, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.2.6': 'A.6.2.2.6 - And some description of an indicator, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.3.2': 'A.6.2.3.2 - And some description of an indicator, it could be pretty long text actually, be ready.'},\n",
    "    'benchmarks': {\n",
    "        'A.6.2.1': 'A.6.2.1 - And some description of an benchmark, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.2': 'A.6.2.2 - And some description of an benchmark, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.3': 'A.6.2.3 - And some description of an benchmark, it could be pretty long text actually, be ready.',\n",
    "    },\n",
    "    'benchmarksalt': {\n",
    "        'A.6.2.1': 'A.6.2.1 - And some description of an benchmark, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.2': 'A.6.2.2 - And some description of an benchmark, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.3': 'A.6.2.3 - And some description of an benchmark, it could be pretty long text actually, be ready.',\n",
    "    },\n",
    "    'benchmarksweighted': {\n",
    "        'A.6.2.1': 'A.6.2.1 - And some description of an benchmark, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.2': 'A.6.2.2 - And some description of an benchmark, it could be pretty long text actually, be ready.',\n",
    "        'A.6.2.3': 'A.6.2.3 - And some description of an benchmark, it could be pretty long text actually, be ready.',\n",
    "    },\n",
    "    'standards' : {\n",
    "        'A.6.2': 'A.6.2 - Some description about a reading standard.'\n",
    "    },\n",
    "    'standardsalt' : {\n",
    "        'A.6.2': 'A.6.2 - Some description about a reading standard.'\n",
    "    },\n",
    "    'standardsweighted' : {\n",
    "        'A.6.2': 'A.6.2 - Some description about a reading standard.'\n",
    "    },\n",
    "    'test' : {\n",
    "        'A.6': 'Reading Grade 6 - English'\n",
    "    },\n",
    "    'testalt' : {\n",
    "        'A.6': 'Reading Grade 6 - English'\n",
    "    },\n",
    "    'testweighted' : {\n",
    "        'A.6': 'Reading Grade 6 - English'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def num_student_for_each_rubric_level(cols_levels, df_metric, metric, weighted=False):\n",
    "    \"\"\"A function to produce various variations of DataFrame used later in Analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cols_levels : List or Dict, required\n",
    "        A list of levels columns for the metric to be processed (e.g. ['A.6.2.1.3Level', etc.] for indicators, OR\n",
    "        A dict of levels percent for the metric to be processed weighted (e.g. \n",
    "        {'A.3.2.1': ['A.3.2.1_L1Percent','A.3.2.1_L2Percent','A.3.2.1_L3Percent','A.3.2.1_L4Percent'], 'A.3.2.2': ['A.3.2.2_L1Percent', etc.})\n",
    "    df_metric : DataFrame, required\n",
    "        The starting DataFrame to process\n",
    "    metric : String, required\n",
    "        A label identifying the metric to be processed (i.e. indicators, benchmarks, standards and test)\n",
    "    weighted: Boolean, required\n",
    "        Whether we producing weighted versions or not\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dfs : Dict\n",
    "        Key value dictionary of dataframes available for later processing\n",
    "    \n",
    "    \"\"\"\n",
    "    print('-------------------------------------------------------------')\n",
    "    print('Number of All Students for Each Rubric Level of '+metric)\n",
    "    print('-------------------------------------------------------------')\n",
    "    \n",
    "    if weighted:\n",
    "        #######################################\n",
    "        # Summary (New weighted version...)\n",
    "        #######################################\n",
    "        metrics = []\n",
    "\n",
    "        for k,v in cols_levels.items():\n",
    "            #print(k)\n",
    "            #print(v)\n",
    "            # where k is the metric (Benchmark, Standard, Whole Test), and\n",
    "            # where v is the Levels percent columsn (*_L1Percent, *_L2Percent, *_L3Percent, *_L4Percent)\n",
    "\n",
    "            df = df_metric[v].copy()\n",
    "            df.loc[k] = df.sum()\n",
    "            df = df.rename(columns = {v[0]: achievement_levels[0], \n",
    "                                      v[1]: achievement_levels[1], \n",
    "                                      v[2]: achievement_levels[2], \n",
    "                                      v[3]: achievement_levels[3]})\n",
    "            df = df.loc[k].to_frame()\n",
    "            metrics.append(df)\n",
    "\n",
    "        df_summary = pd.concat(metrics, axis=1)\n",
    "        print('df_'+metric+'_summary')\n",
    "        display(df_summary)\n",
    "    \n",
    "        #######################################\n",
    "        # Summary by gender (New weighted version...)\n",
    "        #######################################\n",
    "        metrics_gender = []\n",
    "\n",
    "        for k,v in cols_levels.items():\n",
    "            #print(k)\n",
    "            #print(v)\n",
    "            # where k is the metric (Benchmark, Standard, Whole Test), and\n",
    "            # where v is the Levels percent columns (*_L1Percent, *_L2Percent, *_L3Percent, *_L4Percent)\n",
    "\n",
    "            df = df_metric[v+['Gender']].copy()\n",
    "            df = df.pivot(columns='Gender')    \n",
    "            df.loc[k] = df.sum()\n",
    "            df = df.rename(columns = {v[0]: achievement_levels[0], \n",
    "                                      v[1]: achievement_levels[1], \n",
    "                                      v[2]: achievement_levels[2], \n",
    "                                      v[3]: achievement_levels[3]})\n",
    "            df = df.loc[k].to_frame()\n",
    "            df.fillna(0, inplace=True)\n",
    "            metrics_gender.append(df)\n",
    "\n",
    "        df_summary_gender = pd.concat(metrics_gender, axis=1)\n",
    "        df_summary_gender = df_summary_gender.unstack()\n",
    "        print('df_'+metric+'_summary_gender')\n",
    "        display(df_summary_gender)\n",
    "    else:\n",
    "        #######################################\n",
    "        # Summary\n",
    "        #######################################\n",
    "        metrics = []\n",
    "\n",
    "        display(cols_levels)\n",
    "\n",
    "        for m in cols_levels:\n",
    "            df = df_metric[['StudentName',m]].groupby([m]).count()    \n",
    "            df.rename(columns = {'StudentName':m.split('Level')[0]}, inplace = True)\n",
    "            df.index.name = None\n",
    "            metrics.append(df)\n",
    "\n",
    "        df_summary = pd.concat(metrics, axis=1)\n",
    "        print('df_'+metric+'_summary')\n",
    "        display(df_summary)\n",
    "\n",
    "        #######################################\n",
    "        # Summary by gender\n",
    "        #######################################\n",
    "        metric_gender = []\n",
    "\n",
    "        for m in cols_levels:\n",
    "            df = df_metric[['StudentName','Gender',m]].groupby([m, 'Gender']).count()    \n",
    "            df = df.unstack()\n",
    "            df.rename(columns = {'StudentName':m.split('Level')[0]}, inplace = True)\n",
    "            df.index.name = None\n",
    "\n",
    "            metric_gender.append(df)\n",
    "\n",
    "        df_summary_gender = pd.concat(metric_gender, axis=1)\n",
    "        print('df_'+metric+'_summary_gender')\n",
    "        display(df_summary_gender)\n",
    "    \n",
    "    #######################################\n",
    "    # Summary (extended version)\n",
    "    #######################################\n",
    "    df_summary_x = df_summary.rename(columns = descriptions[metric])\n",
    "    print('df_'+metric+'_summary_x')\n",
    "    display(df_summary_x)\n",
    "    \n",
    "    #######################################\n",
    "    # Summary by gender (extended version)\n",
    "    #######################################\n",
    "    df_summary_gender_x = df_summary_gender.rename(columns = descriptions[metric])\n",
    "    print('df_'+metric+'_summary_gender_x')\n",
    "    display(df_summary_gender_x)\n",
    "\n",
    "    #######################################\n",
    "    # Summary including Total row\n",
    "    #######################################\n",
    "    df_summary_tot = df_summary.copy()\n",
    "    df_summary_tot.loc['Total'] = df_summary_tot.sum()\n",
    "    print('df_'+metric+'_summary_tot')\n",
    "    display(df_summary_tot)\n",
    "\n",
    "    #######################################\n",
    "    # Summary including Total row by gender\n",
    "    #######################################\n",
    "    df_summary_gender_tot = df_summary_gender.copy()\n",
    "    df_summary_gender_tot.loc['Total'] = df_summary_gender_tot.sum()\n",
    "    print('df_'+metric+'_summary_gender_tot')\n",
    "    display(df_summary_gender_tot)\n",
    "    \n",
    "    #######################################\n",
    "    # Summary percent\n",
    "    #######################################\n",
    "    df_summary_per = df_summary.copy()\n",
    "    df_summary_per = add_total_in_column_names(df_summary_per, index='Index')\n",
    "    df_summary_per = prepare_for_chart(df_summary_per)\n",
    "    print('df_'+metric+'_summary_per')\n",
    "    display(df_summary_per)\n",
    "\n",
    "    #######################################\n",
    "    # Summary percent by gender percent\n",
    "    #######################################\n",
    "    df_summary_gender_per = df_summary_gender.copy()\n",
    "    df_summary_gender_per = add_total_in_column_names(df_summary_gender_per, index='MultiIndex')\n",
    "    df_summary_gender_per = prepare_for_chart(df_summary_gender_per)\n",
    "    print('df_'+metric+'_summary_gender_per')\n",
    "    display(df_summary_gender_per)\n",
    "\n",
    "    #######################################\n",
    "    # Summary percent (extended version)\n",
    "    #######################################  \n",
    "    df_summary_per_x = df_summary.copy()\n",
    "    df_summary_per_x = df_summary_per_x.rename(columns = descriptions[metric])\n",
    "    df_summary_per_x = add_total_in_column_names(df_summary_per_x, index='Index')\n",
    "    df_summary_per_x = prepare_for_chart(df_summary_per_x)\n",
    "    print('df_'+metric+'_summary_per_x')\n",
    "    display(df_summary_per_x)\n",
    "\n",
    "    #######################################\n",
    "    # Summary percent by gender (extended version)\n",
    "    #######################################\n",
    "    df_summary_gender_per_x = df_summary_gender.copy()\n",
    "    df_summary_gender_per_x = add_total_in_column_names(df_summary_gender_per_x, index='MultiIndex')\n",
    "    df_summary_gender_per_x = df_summary_gender_per_x.rename(columns = descriptions[metric])\n",
    "    df_summary_gender_per_x = prepare_for_chart(df_summary_gender_per_x)\n",
    "    print('df_'+metric+'_summary_gender_per_x')\n",
    "    display(df_summary_gender_per_x)   \n",
    "    \n",
    "    # Troubleshooting\n",
    "    #print('===============================================')\n",
    "    #print('df_'+metric+'_summary_per')\n",
    "    #display(df_summary_per) \n",
    "    \n",
    "    dfs = {         \n",
    "        'df_'+metric+'_summary': df_summary,\n",
    "        'df_'+metric+'_summary_gender': df_summary_gender,\n",
    "        'df_'+metric+'_summary_x' : df_summary_x,\n",
    "        'df_'+metric+'_summary_gender_x' : df_summary_gender_x,\n",
    "        'df_'+metric+'_summary_tot' : df_summary_tot,\n",
    "        'df_'+metric+'_summary_gender_tot' : df_summary_gender_tot,\n",
    "        'df_'+metric+'_summary_per' : df_summary_per,\n",
    "        'df_'+metric+'_summary_gender_per' : df_summary_gender_per,\n",
    "        'df_'+metric+'_summary_per_x' : df_summary_per_x,\n",
    "        'df_'+metric+'_summary_gender_per_x' : df_summary_gender_per_x\n",
    "    }\n",
    "    return dfs\n",
    "\n",
    "##############################################################################\n",
    "# Analysis of Indicators just like in SOE Assessment\n",
    "##############################################################################\n",
    "students_each_rubric_level = num_student_for_each_rubric_level(cols_indicators_levels, df_indicators, 'indicators')\n",
    "\n",
    "##############################################################################\n",
    "# Not in SOE Assessment but included for comparison\n",
    "# This is benchmarks, standards and test analysis but analysed \n",
    "# like SOE analyses indicators\n",
    "##############################################################################\n",
    "students_each_rubric_level.update(num_student_for_each_rubric_level(cols_benchmarks_levels, df_benchmarks, 'benchmarks'))\n",
    "students_each_rubric_level.update(num_student_for_each_rubric_level(cols_benchmarks_levels_alt, df_benchmarks_alt, 'benchmarksalt'))\n",
    "students_each_rubric_level.update(num_student_for_each_rubric_level(benchmarks_levels_percent, df_benchmarks_weighted, 'benchmarksweighted', weighted=True))\n",
    "students_each_rubric_level.update(num_student_for_each_rubric_level(cols_standards_levels, df_standards, 'standards'))\n",
    "students_each_rubric_level.update(num_student_for_each_rubric_level(cols_standards_levels_alt, df_standards_alt, 'standardsalt'))\n",
    "students_each_rubric_level.update(num_student_for_each_rubric_level(standards_levels_percent, df_standards_weighted, 'standardsweighted', weighted=True))\n",
    "students_each_rubric_level.update(num_student_for_each_rubric_level(cols_test_levels, df_test, 'test'))\n",
    "students_each_rubric_level.update(num_student_for_each_rubric_level(cols_test_levels_alt, df_test_alt, 'testalt'))\n",
    "students_each_rubric_level.update(num_student_for_each_rubric_level(test_levels_percent, df_test_weighted, 'testweighted', weighted=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try another alternative to produce level count analysis: Weighted technique\n",
    "\n",
    "df = df_student_results_aggscores.copy()\n",
    "\n",
    "print(\"Benchmarks levels percent\")\n",
    "display(benchmarks_levels_percent)\n",
    "print(\"Benchmarks levels columns\")\n",
    "display(cols_benchmarks_levels)\n",
    "\n",
    "# Sample as previously done...\n",
    "metrics = []\n",
    "\n",
    "for m in cols_benchmarks_levels:\n",
    "    df2 = df[['StudentName',m]].groupby([m]).count()    \n",
    "    df2.rename(columns = {'StudentName':m.split('Level')[0]}, inplace = True)\n",
    "    df2.index.name = None\n",
    "    metrics.append(df2)\n",
    "\n",
    "df3 = pd.concat(metrics, axis=1)\n",
    "print('df_benchmarks_summary')\n",
    "display(df3)\n",
    "\n",
    "# New weighted version\n",
    "metrics = []\n",
    "\n",
    "for k,v in benchmarks_levels_percent.items():\n",
    "    #print(k)\n",
    "    #print(v)\n",
    "    # where k is the metric (Benchmark, Standard, Whole Test), and\n",
    "    # where v is the Levels percent columsn (*_L1Percent, *_L2Percent, *_L3Percent, *_L4Percent)\n",
    "    \n",
    "    cols = benchmarks_levels_percent[k]\n",
    "    df7 = df[cols].copy()\n",
    "    df7.loc[k] = df7.sum()\n",
    "    df7 = df7.rename(columns = {cols[0]: achievement_levels[0], cols[1]: achievement_levels[1], cols[2]: achievement_levels[2], cols[3]: achievement_levels[3]})\n",
    "    df7 = df7.loc[k].to_frame()\n",
    "    metrics.append(df7)\n",
    "\n",
    "df8 = pd.concat(metrics, axis=1)\n",
    "print('df_benchmarks_summary')\n",
    "display(df8)\n",
    "\n",
    "# Sample as previously done by gender...\n",
    "metric_gender = []\n",
    "\n",
    "for m in cols_benchmarks_levels:\n",
    "    df4 = df[['StudentName','Gender',m]].groupby([m, 'Gender']).count()    \n",
    "    df4 = df4.unstack()\n",
    "    df4.rename(columns = {'StudentName':m.split('Level')[0]}, inplace = True)\n",
    "    df4.index.name = None\n",
    "    df4.fillna(0, inplace=True)\n",
    "\n",
    "    metric_gender.append(df4)\n",
    "\n",
    "df5 = pd.concat(metric_gender, axis=1)\n",
    "print('df_benchmark_summary_gender')\n",
    "display(df5)\n",
    "\n",
    "# New weighted version...\n",
    "metrics_gender = []\n",
    "\n",
    "for k,v in benchmarks_levels_percent.items():\n",
    "    #print(k)\n",
    "    #print(v)\n",
    "    # where k is the metric (Benchmark, Standard, Whole Test), and\n",
    "    # where v is the Levels percent columsn (*_L1Percent, *_L2Percent, *_L3Percent, *_L4Percent)\n",
    "\n",
    "    df9 = df[v+['Gender']].copy()\n",
    "    df9 = df9.pivot(columns='Gender')    \n",
    "    df9.loc[k] = df9.sum()\n",
    "    df9 = df9.rename(columns = {v[0]: achievement_levels[0], \n",
    "                                v[1]: achievement_levels[1], \n",
    "                                v[2]: achievement_levels[2], \n",
    "                                v[3]: achievement_levels[3]})\n",
    "    df9 = df9.loc[k].to_frame()\n",
    "    df9.fillna(0, inplace=True)\n",
    "    metrics_gender.append(df9)\n",
    "\n",
    "df10 = pd.concat(metrics_gender, axis=1)\n",
    "df10 = df10.unstack()\n",
    "print('df_benchmarks_summary')\n",
    "display(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Results Sheet (continue)                                                    \n",
    "# Analysis of benchmarks, standards and test just like SOE assessment \n",
    "# (level counts, not students)\n",
    "###############################################################################\n",
    "\n",
    "def level_count_for_each_rubric_level(metric_levels, metric_indicators, df_indicators_summary, metric):\n",
    "    \"\"\"A function to produce various variations of DataFrame used later in Analysis. This is how\n",
    "    Dr. SOE does his analysis on Benchmarks, Standards and Test (not Indicators)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metric_levels : Dict, required\n",
    "        List of levels columns for the metric to be processed (e.g. {'benchmarkX|standardY|test' : ['A.6.2.1.3Level', etc.]}\n",
    "    metric_indicators : Dict, required\n",
    "        List of indicators for the metric to be processed (e.g. {'benchmarkX|standardY|test' : ['A.6.2.1.3', etc.]}    \n",
    "    df_indicators_summary : DataFrame, required\n",
    "        The starting DataFrame to process benchmarks, standards and test DataFrame as SOE does it.\n",
    "    metric : String, required\n",
    "        A label identifying the metric to be processed (i.e. indicators, benchmarks, standards and test)\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dfs : Dict\n",
    "        Key value dictionary of dataframes available for later processing\n",
    "    \n",
    "    \"\"\"\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    print('Level Counts (NOT Students) for All Students for Each Rubric Level of '+metric)\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    \n",
    "    #######################################\n",
    "    # Summary\n",
    "    #######################################\n",
    "    df_summary = df_indicators_summary.copy()\n",
    "    \n",
    "    # Here lies the important difference in how SOE does the analysis for benchmarks, standards and test\n",
    "    # The benchmark's (or standard's or test's) indicators (columns) scores are summed\n",
    "    for m in metric_levels:\n",
    "        indicators = metric_indicators[m]\n",
    "\n",
    "        df_summary[m] = df_summary[indicators].sum(axis=1)\n",
    "        df_summary = df_summary.drop(indicators, axis=1)\n",
    "\n",
    "    if metric=='standards':\n",
    "        df_summary['Whole Test'] = df_summary[list(metric_levels.keys())].sum(axis=1)\n",
    "    \n",
    "    print('df_'+metric+'_summary')\n",
    "    display(df_summary)\n",
    "\n",
    "    #######################################\n",
    "    # Summary (extended version)\n",
    "    #######################################\n",
    "\n",
    "    df_summary_x = df_summary.rename(columns = descriptions[metric])\n",
    "    print('df_'+metric+'_summary_x')\n",
    "    display(df_summary_x)\n",
    "\n",
    "    #######################################\n",
    "    # Summary including Total row\n",
    "    #######################################\n",
    "\n",
    "    df_summary_tot = df_summary.copy()\n",
    "    df_summary_tot.loc['Total'] = df_summary_tot.sum()\n",
    "    print('df_'+metric+'_summary_tot')\n",
    "    display(df_summary_tot)\n",
    "\n",
    "    #######################################\n",
    "    # Summary percent\n",
    "    #######################################\n",
    "\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    print('Level Percents for All Students for Each Rubric Level of '+metric)\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    df_summary_per = df_summary.copy()\n",
    "    df_summary_per = add_total_in_column_names(df_summary_per, index='Index')\n",
    "    df_summary_per = prepare_for_chart(df_summary_per)\n",
    "    print('df_'+metric+'_summary_per')\n",
    "    display(df_summary_per)\n",
    "\n",
    "    #######################################\n",
    "    # Summary percent (extended version)\n",
    "    #######################################\n",
    "    df_summary_per_x = df_summary_x.copy()\n",
    "    df_summary_per_x = add_total_in_column_names(df_summary_per_x, index='Index')\n",
    "    df_summary_per_x = prepare_for_chart(df_summary_per_x)\n",
    "    print('df_'+metric+'_summary_per_x')\n",
    "    display(df_summary_per_x)\n",
    "    \n",
    "    dfs = {\n",
    "        'df_'+metric+'_summary': df_summary, \n",
    "        'df_'+metric+'_summary_x' : df_summary_x,\n",
    "        'df_'+metric+'_summary_tot' : df_summary_tot,\n",
    "        'df_'+metric+'_summary_per' : df_summary_per,\n",
    "        'df_'+metric+'_summary_per_x' : df_summary_per_x,\n",
    "    }\n",
    "    return dfs\n",
    "\n",
    "df_indicators_summary = students_each_rubric_level['df_indicators_summary'].copy()\n",
    "\n",
    "level_count_each_rubric_level_soe = level_count_for_each_rubric_level(benchmarks_indicators_levels, benchmarks_indicators, df_indicators_summary, 'benchmarks')\n",
    "level_count_each_rubric_level_soe.update(level_count_for_each_rubric_level(standards_indicators_levels, standards_indicators, df_indicators_summary, 'standards'))\n",
    "level_count_each_rubric_level_soe.update(level_count_for_each_rubric_level(test_indicators_levels, test_indicators, df_indicators_summary, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Results Sheet (continue)                                                    #\n",
    "###############################################################################\n",
    "\n",
    "exam = df_student_results_analysis['TestName'][0]\n",
    "\n",
    "def prepare_for_plotting(df, metric):\n",
    "    \"\"\"A function that some a couple of transformation preparing for plotting\n",
    "    \n",
    "    Preperations for plotting. This uses the dataframe produced in several of the\n",
    "    above cells. They are all packaged in the following Dicts\n",
    " \n",
    "        * students_each_rubric_level\n",
    "        * level_count_each_rubric_level_soe (gender versions not currently offered)\n",
    "    \n",
    "    And can be accessed with following keys:\n",
    "        * 'df_'+metric+'_summary'\n",
    "        * 'df_'+metric+'_summary_gender'\n",
    "        * 'df_'+metric+'_summary_x'\n",
    "        * 'df_'+metric+'_summary_gender_x'\n",
    "        * 'df_'+metric+'_summary_tot'\n",
    "        * 'df_'+metric+'_summary_gender_tot'\n",
    "        * 'df_'+metric+'_summary_per'\n",
    "        * 'df_'+metric+'_summary_gender_per'\n",
    "        * 'df_'+metric+'_summary_per_x'\n",
    "        * 'df_'+metric+'_summary_gender_per_x'\n",
    "    \n",
    "    For example, \n",
    "        * students_each_rubric_level['df_indicators_summary_per']\n",
    "        * level_count_each_rubric_level_soe['df_benchmarks_summary_per']\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame, required\n",
    "        The DataFrame to prepare for plotting\n",
    "    metric : String, required\n",
    "        A label identifying the metric to be processed (i.e. indicators, benchmarks, standards and test)\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        Could raise unknown error. Implement if it happens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        The DataFrame ready for plotting\n",
    "    \n",
    "    \"\"\"    \n",
    "    df_summary_plot = df.drop('Total', axis='index')\n",
    "    df_summary_plot = df_summary_plot.T\n",
    "    df_summary_plot = df_summary_plot.sort_index()\n",
    "    print('df_'+metric+'_summary_plot')\n",
    "    display(df_summary_plot)\n",
    "    return df_summary_plot\n",
    "\n",
    "students_each_rubric_level_plottable = [i for i in students_each_rubric_level.keys() if 'summary_per' in i] \n",
    "level_count_each_rubric_level_soe_plottable = [i for i in level_count_each_rubric_level_soe.keys() if 'summary_per' in i] \n",
    "\n",
    "students_each_rubric_level_plottable_dfs = {}\n",
    "for df_name in students_each_rubric_level_plottable:\n",
    "    df = prepare_for_plotting(students_each_rubric_level[df_name], df_name.split('_')[1])\n",
    "    students_each_rubric_level_plottable_dfs[df_name] = df\n",
    "\n",
    "level_count_each_rubric_level_soe_plottable_dfs = {}\n",
    "for df_name in level_count_each_rubric_level_soe_plottable:\n",
    "    df = prepare_for_plotting(level_count_each_rubric_level_soe[df_name], df_name.split('_')[1])\n",
    "    level_count_each_rubric_level_soe_plottable_dfs[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Results Sheet (continue)                                                    #\n",
    "# Plotting functions                                                          #\n",
    "###############################################################################\n",
    "\n",
    "# Trying on same grid but having difficulties with long labels\n",
    "#fig = plt.figure(figsize=(30, 4)) #, constrained_layout=True) \n",
    "#gs = gridspec.GridSpec(2, 1) #, width_ratios=[4, 9]) \n",
    "#gs.update(wspace=0.02, hspace=0)\n",
    "\n",
    "##############################\n",
    "# SOE Assessment Style Chart #\n",
    "##############################\n",
    "def plot_soe(df, label='xlabel', dimension='Students', title='N/A'):\n",
    "    \"\"\"A function to plot a DataFrame in SOE style.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame, required\n",
    "        The DataFrame to plot the graph with.\n",
    "    label : String, optional\n",
    "        A string that will be show in the Y axis label\n",
    "    dimension : String, options\n",
    "        A string to modify that X axis label. In general for this plots we have two types of analysis:\n",
    "            - SOE style of \"level counts\" or counting of indicators in a particular benchmark/standard/test at each performance level\n",
    "            - EMIS styles (more common) of counting of students  in a particular benchmark/standard/test at each performance level\n",
    "    \"\"\"\n",
    "    fig1 = plt.figure(figsize=(8, 4)) #, constrained_layout=True) \n",
    "    ax1 = plt.subplot() #gs[0]\n",
    "    df.plot(ax=ax1, kind='bar', stacked=True)\n",
    "\n",
    "    ax1.set_title('Republic of the Marshall Islands\\n{}\\nAll Students of AllSchools\\nSOE Chart Style ({})'.format(exam, title), color='black')\n",
    "    ax1.set_xlabel(label)\n",
    "    ax1.set_ylabel('Percent of '+dimension+' in Each Performance Level')\n",
    "\n",
    "    bars1 = ax1.patches\n",
    "\n",
    "    # Add text to bars\n",
    "    for bar in bars1:\n",
    "        # Find where everything is located\n",
    "        height = bar.get_height()\n",
    "        width = bar.get_width()\n",
    "        x = bar.get_x()\n",
    "        y = bar.get_y()\n",
    "\n",
    "        # The height of the bar is the data value and can be used as the label\n",
    "        label_text =f'{abs(height*100):.0f}%'  # f'{width:.2f}' to format decimal values\n",
    "\n",
    "        label_x = x + 0.45 + width / 2\n",
    "        label_y = y + height / 2\n",
    "\n",
    "        # only plot labels greater than given width\n",
    "        if abs(height) > 0:\n",
    "            ax1.text(label_x, label_y, label_text, ha='center', va='center', fontsize=8)\n",
    "\n",
    "    hatches1 = []\n",
    "    for h in ['----','ooo','////','\\\\\\\\\\\\\\\\']:\n",
    "        for i in range(len(df)):\n",
    "            hatches1.append(h)\n",
    "    colors1 = []\n",
    "    for c in ['#ffffff','#ffffff','#ffffff','#ffffff']:\n",
    "        for i in range(len(df)):\n",
    "            colors1.append(c)\n",
    "    edgecolors1 = []\n",
    "    for ec in ['#0000ff','#800080','#008000','#ff0000']:\n",
    "        for i in range(len(df)):\n",
    "            edgecolors1.append(ec)\n",
    "\n",
    "    for bar, hatch, color, edgecolor in zip(bars1, hatches1, colors1, edgecolors1):\n",
    "        bar.set_color(color)\n",
    "        bar.set_hatch(hatch)\n",
    "        bar.set_edgecolor(edgecolor)\n",
    "\n",
    "    ax1.legend(loc='upper right', bbox_to_anchor=(1.0, 1.35))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "############################\n",
    "# Pacific EMIS Style Chart #\n",
    "############################\n",
    "def plot_emis(df, label='xlabel', dimension='Students', title='N/A'):\n",
    "    \"\"\"A function to plot a DataFrame in EMIS style.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame, required\n",
    "        The DataFrame to plot the graph with.\n",
    "    label : String, optional\n",
    "        A string that will be show in the Y axis label\n",
    "    dimension : String, options\n",
    "        A string to modify that X axis label. In general for this plots we have two types of analysis:\n",
    "            - SOE style of \"level counts\" or counting of indicators in a particular benchmark/standard/test at each performance level\n",
    "            - EMIS styles (more common) of counting of students  in a particular benchmark/standard/test at each performance level\n",
    "    \"\"\"\n",
    "    fig2 = plt.figure(figsize=(8, 6)) #, constrained_layout=True) \n",
    "    ax2 = plt.subplot() #gs[1]\n",
    "    df.plot(ax=ax2, kind='barh', stacked=True)\n",
    "\n",
    "    ax2.set_title('Republic of the Marshall Islands\\n{}\\nAll Students of AllSchools\\nPacific EMIS Chart Style ({})'.format(exam, title), color='black')\n",
    "    ax2.set_xlabel(label)\n",
    "    ax2.set_ylabel('Percent of '+dimension+' in Each Performance Level')\n",
    "\n",
    "    bars2 = ax2.patches\n",
    "\n",
    "    # Add text to bars\n",
    "    for bar in bars2:\n",
    "        # Find where everything is located\n",
    "        height = bar.get_height()\n",
    "        width = bar.get_width()\n",
    "        x = bar.get_x()\n",
    "        y = bar.get_y()\n",
    "\n",
    "        # The height of the bar is the data value and can be used as the label\n",
    "        label_text =f'{abs(width*100):.0f}%'  # f'{width:.2f}' to format decimal values\n",
    "\n",
    "        label_x = x + width / 2\n",
    "        label_y = y + height / 2\n",
    "\n",
    "        # only plot labels greater than given width\n",
    "        if abs(width) > 0:\n",
    "            ax2.text(label_x, label_y, label_text, ha='center', va='center', fontsize=8)\n",
    "\n",
    "    colors2 = []\n",
    "    for c in ['#92d050','#00b050','#ffc000','#ff0000']:\n",
    "        for i in range(len(df)):\n",
    "            colors2.append(c)\n",
    "\n",
    "    for bar, color in zip(bars2, colors2):\n",
    "        bar.set_color(color)\n",
    "\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.0, 1.5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Results Sheet (continue)                                                    #\n",
    "# Plotting anything and everything                                            #\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "# All available DataFrames for plotting are packaged in the following Dicts\n",
    "#  * students_each_rubric_level_plottable_dfs = {}\n",
    "#  * level_count_each_rubric_level_soe_plottable_dfs = {}\n",
    "# For example, access one like this students_each_rubric_level_plottable_dfs['df_indicators_summary_per']\n",
    "print(\"Student count at each rubric level available DataFrames:\")\n",
    "pp.pprint(list(students_each_rubric_level_plottable_dfs.keys()))\n",
    "print()\n",
    "print(\"Level/Indicator counts (not Students) at each rubric level available DataFrames:\")\n",
    "# Following does not have an indicators dataframe since those as merely computed on student\n",
    "# and thus only part of previous list above.\n",
    "pp.pprint(list(level_count_each_rubric_level_soe_plottable_dfs.keys()))\n",
    "\n",
    "###############################################################################\n",
    "# Reminders:\n",
    "# students_each_rubric_level_plottable_dfs['df_benchmarks_summary_*'] produce a student count at each rubric level. The levels are calculated as in SOE (e.g. use of A.6.2.1Level columns based on A.6.2.1_L1Percent columns)\n",
    "# students_each_rubric_level_plottable_dfs['df_benchmarksalt_summary_*'] produce a student count at each rubric level. The levels are calculated based on 4 equal bins from all their respective items directly\n",
    "# plot_soe(level_count_each_rubric_level_soe_plottable_dfs['df_benchmarks_summary_*']) produces a level count (not sutdent) at each rubric level. This is how SOE produces benchmarks, standards and test analysis\n",
    "#\n",
    "# Note that they end up all similar but different results\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "# Indicators analysis\n",
    "###############################################################################\n",
    "\n",
    "# Everybody does this one the same hence not all the variations like benchmarks, standards and test are included\n",
    "#plot_soe(students_each_rubric_level_plottable_dfs['df_indicators_summary_per'], 'Indicators', 'Student', 'Item Count Method') # Student count at each rubric level (SOE Chart Style)\n",
    "#plot_emis(students_each_rubric_level_plottable_dfs['df_indicators_summary_per_x'], 'Indicators', 'Student', 'Item Count Method') # Student count at each rubric level (EMIS Chart Style)\n",
    "\n",
    "###############################################################################\n",
    "# Benchmarks analysis\n",
    "###############################################################################\n",
    "\n",
    "#plot_soe(students_each_rubric_level_plottable_dfs['df_benchmarks_summary_per'], 'Benchmarks', 'Student', 'Level Percentage Method') # Student count at each rubric level SOE Extension rules (SOE Chart style)\n",
    "#plot_soe(students_each_rubric_level_plottable_dfs['df_benchmarksalt_summary_per'], 'Benchmarks', 'Student', 'Item Count Method') # Student count at each rubric level ItemCount rule by passing indicator (SOE Chart Style)\n",
    "#plot_soe(students_each_rubric_level_plottable_dfs['df_benchmarksweighted_summary_per'], 'Benchmarks', 'Indicators (Level count)', 'Weighted Method') # Level count count at each rubric level using weighting technique (SOE Chart Style)\n",
    "#plot_soe(level_count_each_rubric_level_soe_plottable_dfs['df_benchmarks_summary_per'], 'Benchmarks', 'Indicators (Level count)', 'Indicators Level Count Method') # SOE's level count technique (SOE Chart style)\n",
    "#plot_emis(students_each_rubric_level_plottable_dfs['df_benchmarks_summary_per_x'], 'Benchmarks', 'Student', 'Level Percentage Method') # Student count at each rubric level ItemCount rule by passing indicator (EMIS Chart Style)\n",
    "plot_emis(students_each_rubric_level_plottable_dfs['df_benchmarksalt_summary_per_x'], 'Benchmarks', 'Student', 'Item Count Method') # Student count at each rubric level SOE Extension rules (EMIS Chart style)\n",
    "plot_emis(students_each_rubric_level_plottable_dfs['df_benchmarksweighted_summary_per_x'], 'Benchmarks', 'Indicators (Level count)', 'Weighted Method') # Level count count at each rubric level using weighting technique (EMIS Chart style)\n",
    "plot_emis(level_count_each_rubric_level_soe_plottable_dfs['df_benchmarks_summary_per_x'], 'Benchmarks', 'Indicators (Level count)', 'Indicators Level Count Method') # SOE's level count technique (EMIS Chart style)\n",
    "\n",
    "###############################################################################\n",
    "# Standards analysis\n",
    "###############################################################################\n",
    "\n",
    "#plot_soe(students_each_rubric_level_plottable_dfs['df_standards_summary_per'], 'Standard', 'Student', 'Level Percentage Method') # Student count at each rubric level SOE Extension rules (SOE Chart style)\n",
    "#plot_soe(students_each_rubric_level_plottable_dfs['df_standardsalt_summary_per'], 'Standard', 'Student', 'Item Count Method') # Student count at each rubric level ItemCount rule by passing indicator (SOE Chart Style)\n",
    "#plot_soe(students_each_rubric_level_plottable_dfs['df_standardsweighted_summary_per'], 'Standard', 'Indicators (Level count)', 'Weighted Method') # Level count count at each rubric level using weighting technique (SOE Chart Style)\n",
    "#plot_soe(level_count_each_rubric_level_soe_plottable_dfs['df_standards_summary_per'], 'Standard', 'Indicators (Level count)', 'Indicators Level Count Method') # SOE's level count technique (SOE Chart style)\n",
    "#plot_emis(students_each_rubric_level_plottable_dfs['df_standards_summary_per_x'], 'Standard', 'Student', 'Level Percentage Method') # Student count at each rubric level ItemCount rule by passing indicator (EMIS Chart Style)\n",
    "#plot_emis(students_each_rubric_level_plottable_dfs['df_standardsalt_summary_per_x'], 'Standard', 'Student', 'Item Count Method') # Student count at each rubric level SOE Extension rules (EMIS Chart style)\n",
    "#plot_emis(students_each_rubric_level_plottable_dfs['df_standardsweighted_summary_per_x'], 'Standard', 'Indicators (Level count)', 'Weighted Method') # Level count count at each rubric level using weighting technique (EMIS Chart style)\n",
    "#plot_emis(level_count_each_rubric_level_soe_plottable_dfs['df_standards_summary_per_x'], 'Standard', 'Indicators (Level count)', 'Indicators Level Count Method') # SOE's level count technique (EMIS Chart style)\n",
    "\n",
    "###############################################################################\n",
    "# Test analysis\n",
    "###############################################################################\n",
    "\n",
    "#plot_soe(students_each_rubric_level_plottable_dfs['df_test_summary_per'], 'Whole test', 'Student', 'Level Percentage Method') # Student count at each rubric level SOE Extension rules (SOE Chart style)\n",
    "#plot_soe(students_each_rubric_level_plottable_dfs['df_testalt_summary_per'], 'Whole test', 'Student', 'Item Count Method') # Student count at each rubric level ItemCount rule by passing indicator (SOE Chart Style)\n",
    "#plot_soe(students_each_rubric_level_plottable_dfs['df_testweighted_summary_per'], 'Whole test', 'Indicators (Level count)', 'Weighted Method') # Level count count at each rubric level using weighting technique (SOE Chart Style)\n",
    "#plot_soe(level_count_each_rubric_level_soe_plottable_dfs['df_test_summary_per'], 'Whole test', 'Indicators (Level count)', 'Indicators Level Count Method') # SOE's level count technique (SOE Chart style)\n",
    "#plot_emis(students_each_rubric_level_plottable_dfs['df_test_summary_per_x'], 'Whole test', 'Student', 'Level Percentage Method') # Student count at each rubric level ItemCount rule by passing indicator (EMIS Chart Style)\n",
    "#plot_emis(students_each_rubric_level_plottable_dfs['df_testalt_summary_per_x'], 'Whole test', 'Student', 'Item Count Method') # Student count at each rubric level SOE Extension rules (EMIS Chart style)\n",
    "#plot_emis(students_each_rubric_level_plottable_dfs['df_testweighted_summary_per_x'], 'Whole test', 'Indicators (Level count)', 'Weighted Method') # Level count count at each rubric level using weighting technique (EMIS Chart style)\n",
    "#plot_emis(level_count_each_rubric_level_soe_plottable_dfs['df_test_summary_per_x'], 'Whole test', 'Indicators (Level count)', 'Indicators Level Count Method') # SOE's level count technique (EMIS Chart style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write various DataFrame into Excel to examine (testing)\n",
    "filename = os.path.join(cwd, 'data/RMI/soe-assessment-workbook.xlsx')\n",
    "with pd.ExcelWriter(filename) as writer:\n",
    "    # add DataFrames you want to write to Excel here\n",
    "    df_student_results.to_excel(writer, index=False, sheet_name='Responses', engine='openpyxl')\n",
    "    df_student_results_scores.to_excel(writer, index=False, sheet_name='Scores', engine='openpyxl')\n",
    "    df_student_results_aggscores.to_excel(writer, index=False, sheet_name='AggregateScores', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-costa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-active",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-schedule",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
